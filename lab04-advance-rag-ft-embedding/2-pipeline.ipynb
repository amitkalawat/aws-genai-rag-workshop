{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3b8064-f819-45c9-84d2-841f2e540cb4",
   "metadata": {},
   "source": [
    "## Fine-tune Embedding Models\n",
    "\n",
    "Fine-tuning the embedding model is a critical step in enhancing the performance of RAG systems. These systems rely on retrieving relevant information from a corpus to augment the language model's generation capabilities. However, pre-trained embedding models are often trained on general-purpose datasets, which may not accurately capture the nuances and semantics specific to a particular domain or use case. Fine-tuning the embedding model on domain-specific data allows the RAG system to adapt to the target domain, improving the relevance and accuracy of retrieved information. \n",
    "\n",
    "In this notebook, we will connect the manual steps from previous notebook into a continuous SageMaker Pipelines. That way, you can establish the MLOps best practice and finetune sentencetransformer models in a consistent and reliable manner.\n",
    "\n",
    "![Fine tuning pipeline](../static/finetune-embedding-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96615b03",
   "metadata": {},
   "source": [
    "## Pre-req\n",
    "You must run the `[workshop_setup.ipynb]`(../lab00-setup/workshop_setup.ipynb) notebook in `lab00-setup` before starting this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf342e7-163b-4cff-ba81-2eaa01cc2360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.warn(\"Warning: if you did not run lab00-setup, please go back and run the lab00 notebook\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff1cf9a-b3d2-4222-8d8a-49d0d08103bb",
   "metadata": {},
   "source": [
    "## Load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c586f6e-c488-4350-9b0d-d9d8a4d3d188",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r train_s3_path\n",
    "%store -r valid_s3_path\n",
    "%store -r lab04_prefix\n",
    "%store -r model_id\n",
    "\n",
    "## check all 4 values are printed and do not fail\n",
    "print(train_s3_path)\n",
    "print(valid_s3_path)\n",
    "print(lab04_prefix)\n",
    "print(model_id)\n",
    "\n",
    "prefix = lab04_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea5c32",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231ab36-f97e-4f80-8e4b-e1dc084fe9b8",
   "metadata": {},
   "source": [
    "### > Surpress warning messages in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da32a475-929b-4517-bd71-94ba35732685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.getLogger().setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094866c-7055-4413-bf56-96c0b7d91684",
   "metadata": {},
   "source": [
    "### > intialize or retrieve global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f79deb-3721-49fe-843d-3bd867e5c1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "\n",
    "from sagemaker.workflow.parameters import ParameterString, ParameterFloat, ParameterInteger, ParameterBoolean\n",
    "from sagemaker.pytorch.processing import PyTorchProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "pipeline_session = PipelineSession()\n",
    "region = pipeline_session.boto_region_name\n",
    "default_bucket = pipeline_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "\n",
    "training_instance_count = 1\n",
    "evaluation_instance_count = 1\n",
    "evaluation_instance_type = \"ml.m5.2xlarge\"\n",
    "training_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "model_output_s3_loc = f\"s3://{default_bucket}/data/finetuning-{model_id.replace('/', '-')}/model\"\n",
    "\n",
    "base_model_pkg_group_name = name_from_base(model_id.replace('/', '-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ad26b-3d1d-47c9-a4b3-29e951877ab8",
   "metadata": {},
   "source": [
    "Setup setup caching to 12 hours, this is great way to improve the speed of pipeline debugging. With caching turned on, the pipeline will not rerun steps if the input did not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176fd8de-c93b-4995-9cfb-e78ec264a43f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"T12H\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0a520-4809-4c02-85c7-c4024a6cf571",
   "metadata": {},
   "source": [
    "# Define Parameters to parametize SageMaker Pipeline Executions\n",
    "Define Pipeline parameters that you can use to parametrize the pipeline. Parameters enable custom pipeline executions and schedules without having to modify the Pipeline definition.\n",
    "\n",
    "The supported parameter types include:\n",
    "\n",
    "* ParameterString - represents a str Python type\n",
    "* ParameterInteger - represents an int Python type\n",
    "* ParameterFloat - represents a float Python type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b77c4f9-6b3c-4bfb-b566-3a188ede239e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "# model id\n",
    "model_id_param = ParameterString(name=\"ModelId\", default_value=model_id)\n",
    "# epochs\n",
    "epochs_param = ParameterInteger(name=\"Epochs\", default_value=1)\n",
    "# batch size\n",
    "batch_size_param = ParameterInteger(name=\"BatchSize\", default_value=10)\n",
    "# eval steps\n",
    "evaluation_steps_param = ParameterInteger(name=\"EvalSteps\", default_value=50)\n",
    "\n",
    "#data locations\n",
    "training_dataset_s3_loc_param = ParameterString(name=\"TrainingDatasetS3LocParam\", default_value=train_s3_path)\n",
    "eval_dataset_s3_loc_param = ParameterString(name=\"EvalDatasetS3LocParam\", default_value=valid_s3_path)\n",
    "model_output_s3_loc_param = ParameterString(name=\"ModelOutputS3LocParam\", default_value=model_output_s3_loc)\n",
    "\n",
    "#instance type\n",
    "training_job_instance_type_param = ParameterString(name=\"TrainingJobInstanceType\", default_value=training_instance_type)\n",
    "eval_job_instance_type_param = ParameterString(name=\"EvaluationJobInstanceType\", default_value=evaluation_instance_type)\n",
    "\n",
    "base_model_group_name_param = ParameterString(name=\"BaseModelRegistryGroupName\", default_value=base_model_pkg_group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba9196-ae51-427d-8264-83539f2a9f09",
   "metadata": {},
   "source": [
    "Training Step\n",
    "In this section, use define a training step to finetune an embedding model on the given dataset. Configure an Estimator for the HuggingFace and the input dataset. A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to model_dir so that it can be hosted later.\n",
    "\n",
    "The model path where the models from training are saved is also specified.\n",
    "\n",
    "Note: the instance_type parameter may be used in multiple places in the pipeline. In this case, the instance_type is passed into the estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78bb36a-ad22-4a42-bd90-ba092027bb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id_param,                             # pre-trained model\n",
    "    \"epochs\": epochs_param,\n",
    "    \"batch_size\": batch_size_param,\n",
    "    \"evaluation_steps\": evaluation_steps_param\n",
    "}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point=\"train.py\",                                 # train script\n",
    "    source_dir=\"scripts\",                                   # directory which includes all the files needed for training\n",
    "    instance_type=training_job_instance_type_param,         # instances type used for the training job\n",
    "    instance_count=1,                                       # the number of instances used for training\n",
    "    base_job_name=name_from_base(f\"{prefix}-training-step\"),          # the name of the training job\n",
    "    role=role,                                              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    volume_size=100,                                        # the size of the EBS volume in GB\n",
    "    transformers_version=\"4.28\",                            # the transformers version used in the training job\n",
    "    pytorch_version=\"2.0\",                                  # the pytorch_version version used in the training job\n",
    "    py_version=\"py310\",                                     # the python version used in the training job\n",
    "    hyperparameters=hyperparameters,                        # the hyperparameters passed to the training job\n",
    "    environment={\"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\"},   # set env variable to cache models in /tmp\n",
    "    sagemaker_session=pipeline_session,                     # specifies a sagemaker session object\n",
    "    output_path=model_output_s3_loc_param                   # s3 location for model artifact\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3873c027-9740-4ce4-b420-37a717764400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\"train\": training_dataset_s3_loc_param, \"valid\": eval_dataset_s3_loc_param}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "train_args = huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7cee9d-5175-4d8b-aaae-e101cc846689",
   "metadata": {},
   "source": [
    "Define the Training step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df1795-d404-423e-8e19-e1c3509ef925",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"EmbeddingTrain\",\n",
    "    step_args=train_args,\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878611e6-95be-46eb-9514-60e80d7b4234",
   "metadata": {},
   "source": [
    "# Define an Evlatuion Step\n",
    "A processing step is used for triggering a processing job for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983b57d8-893c-4351-ad5b-8cce4af98c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "# Initialize the HuggingFaceProcessor\n",
    "ptp = PyTorchProcessor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=eval_job_instance_type_param,\n",
    "    # transformers_version=\"4.28\",\n",
    "    framework_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    base_job_name=name_from_base(f\"{prefix}-evaluation-step\"),\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69819d57-c24b-4c2c-ade3-bd6264de240c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "# Run the processing job\n",
    "step_args = ptp.run(\n",
    "    code='evaluation.py',\n",
    "    source_dir='pipeline',\n",
    "    arguments=[\"base-model-id\", \"sentence-transformers/msmarco-bert-base-dot-v5\", \n",
    "               \"--model-file\", \"model.tar.gz\",\n",
    "               \"--test-file\", \"val_dataset.json\"],\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            input_name=\"data\",\n",
    "            source=eval_dataset_s3_loc_param,\n",
    "            destination='/opt/ml/processing/input/data/'\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            input_name=\"model\",\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"evaluation\",\n",
    "            source=\"/opt/ml/processing/evaluation\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=f\"{prefix}-evaluation-report\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"EmbeddingEvaluation\",\n",
    "    step_args=step_args,\n",
    "    property_files=[evaluation_report],\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39120a69-158f-480c-bf5e-2ce25cb7db38",
   "metadata": {},
   "source": [
    "# Register Model\n",
    "SageMaker Model Registry supports the following features and functionality:\n",
    "\n",
    "* Catalog models for production.\n",
    "* Manage model versions. \n",
    "* Associate metadata, such as training metrics, with a model.\n",
    "* Manage the approval status of a model.\n",
    "* Deploy models to production.\n",
    "* Automate model deployment with CI/CD.\n",
    "\n",
    "In this workshop, we are going to register the finetuned embedding model as a model package using SageMaker Model Registry. \n",
    "\n",
    "A model package is an abstraction of reusable model artifacts that packages all ingredients required for inference. \n",
    "Primarily, it consists of an inference specification that defines the inference image to use along with an optional model weights location.\n",
    "\n",
    "A model package group is a collection of model packages. A model package group can be created for a specific ML business problem, and new versions of the model packages can be added to it. Typically, customers are expected to create a ModelPackageGroup for a SageMaker pipeline so that model package versions can be added to the group for every SageMaker Pipeline run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f96f0f-5220-4a79-b356-5522c7af8310",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "import json\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "# retrieve the llm image uri\n",
    "triton_image=f\"785573368785.dkr.ecr.{region}.amazonaws.com/sagemaker-tritonserver:22.12-py3\"\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {triton_image}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.2xlarge\"\n",
    "number_of_gpu = 1\n",
    "health_check_timeout = 3600\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    # image_uri=triton_image,\n",
    "    transformers_version=\"4.28\",\n",
    "    pytorch_version=\"2.0\",\n",
    "    py_version=\"py310\",\n",
    "    model_server_workers=1,\n",
    "    role=role,\n",
    "    name=name_from_base(model_id.replace('/', '-')),\n",
    "    sagemaker_session=pipeline_session\n",
    ")\n",
    "\n",
    "create_step_args = huggingface_model.create(instance_type=inference_instance_type)\n",
    "step_create_model = ModelStep(\n",
    "    name=\"CreateModel\",\n",
    "    step_args=create_step_args,\n",
    "    depends_on=[step_eval]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40913a-bd45-4e00-812a-335f5e5bcfc6",
   "metadata": {},
   "source": [
    "# Model Metrics\n",
    "To capture the model training and evalution metrics from a SageMaker Training job, we use a `ModelMetrics` class. We captured the model evaluation metrics in a `evaluation.json`, stored in the specified S3 location. With that information, we create a `ModelMetrics` object to incl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e97cb-22e9-4bba-921c-efa4f7f4f602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "import os \n",
    "\n",
    "model_package_group_name = f\"{model_id.replace('/', '-')}-finetuned\"\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "                step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "            ),\n",
    "            content_type=\"application/json\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478add21-c561-4e45-bcfb-caa5ad29372b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "register_args = huggingface_model.register(\n",
    "    content_types=[\"application/json\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\n",
    "        \"ml.p2.16xlarge\",\n",
    "        \"ml.p3.16xlarge\",\n",
    "        \"ml.g4dn.4xlarge\",\n",
    "        \"ml.g4dn.8xlarge\",\n",
    "        \"ml.g4dn.12xlarge\",\n",
    "        \"ml.g4dn.16xlarge\",\n",
    "        \"ml.g5.2xlarge\",\n",
    "        \"ml.g5.12xlarge\",\n",
    "    ],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    customer_metadata_properties={\"training-image-uri\": huggingface_estimator.training_image_uri()}, #Store the training image url\n",
    "    approval_status=\"PendingManualApproval\",\n",
    "    model_metrics=model_metrics\n",
    ")\n",
    "step_register = ModelStep(name=\"RegisterModel\",\n",
    "                          step_args=register_args,\n",
    "                          depends_on=[step_eval, step_create_model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21994953-c5ad-4edd-bbf4-eadbfd50bf14",
   "metadata": {},
   "source": [
    "# Define a Pipeline of Parameters and Steps \n",
    "In this section, we combine all the steps into a Pipeline so it can be executed.\n",
    "A pipeline requires a name, parameters, and steps. Names must be unique within an (account, region) pair.\n",
    "\n",
    "Note:\n",
    "\n",
    "* All the parameters used in the definitions must be present.\n",
    "* Steps passed into the pipeline do not have to be listed in the order of execution. The SageMaker Pipeline service resolves the data dependency DAG as steps for the execution to complete.\n",
    "* Steps must be unique to across the pipeline step list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539120a2-de90-4c61-8d42-67db934947fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.workflow.pipeline_experiment_config import PipelineExperimentConfig\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=f\"{prefix}-pipeline\",\n",
    "    parameters=[\n",
    "        model_id_param,\n",
    "        epochs_param,\n",
    "        batch_size_param,\n",
    "        evaluation_steps_param,\n",
    "        training_dataset_s3_loc_param,\n",
    "        eval_dataset_s3_loc_param,\n",
    "        model_output_s3_loc_param,\n",
    "        training_job_instance_type_param,\n",
    "        eval_job_instance_type_param,\n",
    "        base_model_group_name_param\n",
    "    ],\n",
    "    steps=[step_train, step_eval, step_create_model, step_register],\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a316047-205b-42a5-b69f-8a69efc9f1f9",
   "metadata": {},
   "source": [
    "## Examining the pipeline definition\n",
    "The JSON of the pipeline definition can be examined to confirm the pipeline is well-defined and the parameters and step properties resolve correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115d2ac-75d3-4ea6-a277-63d04aadee7a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a64a3-a342-4040-98ed-09d05eb428a1",
   "metadata": {},
   "source": [
    "### > Start the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458846bc-5c30-45ab-bbc5-bdb53a38ca05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef57c865-e0e5-4b21-87ba-7664d14210a1",
   "metadata": {},
   "source": [
    "## Access SageMaker Pipelines from Studio\n",
    "\n",
    "You can access your pipeline execution from SageMaker Studio. See below screenshots for details.\n",
    "\n",
    "![Fine tuning pipeline_studio1](../static/Fine-tuning-pipeline_studio1.png)\n",
    "\n",
    "![Fine tuning pipeline_studio2](../static/Fine-tuning-pipeline_studio2.png)\n",
    "\n",
    "![Fine tuning pipeline_studio3](../static/Fine-tuning-pipeline_studio3.png)\n",
    "\n",
    "![Fine tuning pipeline_studio4](../static/Fine-tuning-pipeline_studio4.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115b01d7-6752-4c9a-a099-5c9eb0172fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

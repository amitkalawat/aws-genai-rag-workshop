{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1622b30c-ff9a-4768-8249-0a0c8dbcab37",
   "metadata": {},
   "source": [
    "## Naive RAG Lab\n",
    "\n",
    "In this lab, we will build a naive RAG solution leveraging pre-trained foundation models and the knowledge base feature from Amazon Bedrock. You can do this lab directly in [Bedrock console](https://console.aws.amazon.com/bedrock/) in just a few clicks, but running the notebook will give you a better understanding of the various components that makes up a RAG system.\n",
    "\n",
    "![Naive RAG](../static/naive-rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7470e9-53f6-4714-81b0-50b9cbfdbf26",
   "metadata": {},
   "source": [
    "## Pre-req\n",
    "You must run following cells in notebook [lab00](../lab00-setup/):\n",
    "\n",
    "* Setup for all Labs\n",
    "* Initial Setup Lab01\n",
    "* Initial Setup Lab02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89d74e-6e82-4f99-bcf3-1bbe58aca686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.warn(\"Warning: if you did not run lab01, please go back and run the lab00 notebook up to Initial Setup Lab02\") \n",
    "\n",
    "# bucket and parameter stored from Initial setup lab01\n",
    "%store -r bucket\n",
    "%store -r prefix\n",
    "%store -r data_dir\n",
    "%store -r yml_dir\n",
    "%store -r uml_dir\n",
    "\n",
    "# vector parameters stored from Initial setup lab02\n",
    "%store -r vector_store_name\n",
    "%store -r vector_collection_arn\n",
    "%store -r vector_collection_id\n",
    "%store -r vector_host"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259d2bc-c6c9-4df7-b6b7-e17b9d4b4918",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380e0b2-0589-4205-9758-6da9a67a1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import random\n",
    "import pprint as pp\n",
    "import uuid\n",
    "import json\n",
    "from retrying import retry\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "# auth for opensearch\n",
    "boto3_session = boto3.Session()\n",
    "region_name = boto3_session.region_name\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "credentials = boto3_session.get_credentials()\n",
    "\n",
    "# opensearch service\n",
    "service = 'aoss'\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db990f08-015e-465b-9267-69d9738f4d10",
   "metadata": {},
   "source": [
    "## Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Step 1 - Create OSS policies and collection\n",
    "Firt of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your applicationâ€”without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693b588-e7cd-4ba3-9ce1-fb5d7599fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = random.randrange(200, 900)\n",
    "index_name = f\"swagger-api-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name=bucket)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574555e4-786a-4179-a7d6-17c1f31a3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss(vector_store_name=vector_store_name,\n",
    "                       aoss_client=aoss_client,\n",
    "                       bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe47de-4852-482d-97da-110265e90671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection already created in Lab00\n",
    "print(vector_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176be68c-c749-4df1-a998-663889acabfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create oss policy and attach it to Bedrock execution role\n",
    "create_oss_policy_attach_bedrock_execution_role(collection_id=vector_collection_id,\n",
    "                                                bedrock_kb_execution_role=bedrock_kb_execution_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35d671-1aa4-415b-87f0-6beb336fcfc0",
   "metadata": {},
   "source": [
    "### Step 2 - Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625ad3f-9822-4ca7-9fec-3aa93c7f305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\"\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536,\n",
    "            \"method\": {\n",
    "                \"name\": \"hnsw\",\n",
    "                \"space_type\": \"innerproduct\",\n",
    "                \"engine\": \"faiss\",\n",
    "                \"parameters\": {\n",
    "                  \"ef_construction\": 256,\n",
    "                  \"m\": 48\n",
    "                }\n",
    "             }\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         \n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': vector_host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23e0b4a-1189-477a-84ab-de83741169ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index\n",
    "response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "print('\\nCreating index:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde8bac3-2f34-4260-a26e-2c0efbb94da4",
   "metadata": {},
   "source": [
    "## Create Knowledge Base\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the s3 configuration, which will be used to create the data source object later.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0b3cf-77f6-4a1b-97cd-a576575beb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": vector_collection_arn,\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"NONE\",\n",
    "}\n",
    "\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket}\",\n",
    "    \"inclusionPrefixes\":[f\"{prefix}/yml_questions/\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "kb_name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Swagger OpenAPI knowledge base.\"\n",
    "roleArn = bedrock_kb_execution_role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c12d9fc-a1e9-4607-9d7c-dbcd9b6b7f12",
   "metadata": {},
   "source": [
    "Provide the above configurations as input to the `create_knowledge_base` method, which will create the Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f1013c-a0de-4c69-8308-9de70c20f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = kb_name,\n",
    "        description = description,\n",
    "        roleArn = roleArn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d81d1-b296-4a7a-bef3-22a7452b54d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b97f45-49e0-440e-be3c-10b4d1fe7757",
   "metadata": {},
   "source": [
    "Next we need to create a data source, which will be associated with the knowledge base created above. Once the data source is ready, we can then start to ingest the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa977390-1cbb-4b82-8626-7f0300033ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b2d1b-6708-4dd0-a828-82d4ee5acf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = kb_name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":s3Configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(20)\n",
    "pp.pprint(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885a1a07-68bd-4472-aa0f-e3ecc541aa75",
   "metadata": {},
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source is created, we can start the ingestion job.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d89ee2c-4757-4ab0-a4fe-df4cb810cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an ingestion job\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d27676-0634-4005-bdc7-95b79e7f36fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235dae5-99be-4af2-8c33-db4c6c101830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "  get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "  job = get_job_response[\"ingestionJob\"]\n",
    "pp.pprint(job)\n",
    "time.sleep(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522280f4-7246-4295-a9cf-361bc9f8a79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "%store kb_id\n",
    "pp.pprint(kb_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd0c915-9eae-41f8-8f55-0a08ee148496",
   "metadata": {},
   "source": [
    "## Test the knowledge base\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da746fb-6bd8-470e-aaae-d7be31ce2acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out KB using RetrieveAndGenerate API\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\" # try with both claude instant as well as claude-v2. for claude v2 - \"anthropic.claude-v2\"\n",
    "model_arn = f'arn:aws:bedrock:{region_name}::foundation-model/{model_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fa3c4d-6429-47c9-be7b-f461fae74b91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "query = \"How do I add a new pet using the petstore api? Can you generate a test code in python?\"\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        'text': query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "generated_text = response['output']['text']\n",
    "\n",
    "display(Markdown(generated_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ec9b91-5bc9-400c-9e2a-a5cd7ed6ef89",
   "metadata": {},
   "source": [
    "## > Create a Simple Chat Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f412fd1-edcf-4a91-904b-2ffa77018950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "class ChatUX:\n",
    "    \"\"\" A chat UX using IPWidgets\n",
    "    \"\"\"\n",
    "    def __init__(self, qa):\n",
    "        self.qa = qa\n",
    "        self.name = None\n",
    "        self.b=None\n",
    "        self.out = ipw.Output()\n",
    "        self.session_id = None\n",
    "\n",
    "    def start_chat(self):\n",
    "        print(\"Let's chat!\")\n",
    "        display(self.out)\n",
    "        self.chat(None)\n",
    "\n",
    "    def chat(self, _):\n",
    "        if self.name is None:\n",
    "            prompt = \"\"\n",
    "        else:\n",
    "            prompt = self.name.value\n",
    "        if 'q' == prompt or 'quit' == prompt or 'Q' == prompt:\n",
    "            print(\"Thank you , that was a nice chat !!\")\n",
    "            return\n",
    "        elif len(prompt) > 0:\n",
    "            with self.out:\n",
    "                thinking = ipw.Label(value=f\"Thinking...\")\n",
    "                display(thinking)\n",
    "                try:\n",
    "                    if self.session_id:\n",
    "                        response = self.qa.retrieve_and_generate(\n",
    "                            sessionId=self.session_id,\n",
    "                            input={\n",
    "                                'text': prompt\n",
    "                            },\n",
    "                            retrieveAndGenerateConfiguration={\n",
    "                                'type': 'KNOWLEDGE_BASE',\n",
    "                                'knowledgeBaseConfiguration': {\n",
    "                                    'knowledgeBaseId': kb_id,\n",
    "                                    'modelArn': model_arn\n",
    "                                }\n",
    "                            }\n",
    "                        )\n",
    "                    else:\n",
    "                        response = self.qa.retrieve_and_generate(\n",
    "                            input={\n",
    "                                'text': prompt\n",
    "                            },\n",
    "                            retrieveAndGenerateConfiguration={\n",
    "                                'type': 'KNOWLEDGE_BASE',\n",
    "                                'knowledgeBaseConfiguration': {\n",
    "                                    'knowledgeBaseId': kb_id,\n",
    "                                    'modelArn': model_arn\n",
    "                                }\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    self.session_id = response['sessionId']\n",
    "                    result = response['output']['text']\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    result = \"No answer\"\n",
    "                thinking.value=\"\"\n",
    "                print(f\"AI: {result}\")\n",
    "                self.name.disabled = True\n",
    "                self.b.disabled = True\n",
    "                self.name = None\n",
    "\n",
    "        if self.name is None:\n",
    "            with self.out:\n",
    "                self.name = ipw.Text(description=\"You: \", placeholder='q to quit')\n",
    "                self.b = ipw.Button(description=\"Send\")\n",
    "                self.b.on_click(self.chat)\n",
    "                display(ipw.Box(children=(self.name, self.b)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac85427-eef2-488b-8fff-423c2c32ec07",
   "metadata": {},
   "source": [
    "### > Sample Questions\n",
    "- How many routes in flowerstore?\n",
    "- How do I create a new pet in petstore?\n",
    "- What is the response code if a pet is not available?\n",
    "- How do I delete a book from bookstore?\n",
    "- Can you generate a sample code in python to create a new author in bookstore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9de5c6-505c-45de-8c44-0fdc71a4605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatUX(bedrock_agent_runtime_client)\n",
    "chat.start_chat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6d3191-558a-4a55-b78d-5a0e026a6fcc",
   "metadata": {},
   "source": [
    "### > Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9309589f-9d32-48b5-a5cf-d8d005299485",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

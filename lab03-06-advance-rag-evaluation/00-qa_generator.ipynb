{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d844bcb-5c27-451e-a9d0-07a86c417501",
   "metadata": {},
   "source": [
    "# Use LLM to Automatically Generate Question And Answer From Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d1b755-0f23-40b0-8194-8343e6da0ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import urllib.request\n",
    "import math\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f614fe-659f-4b73-b08c-0179d52a5e61",
   "metadata": {},
   "source": [
    "Setup boto3 clients for Bedrock model invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f597e5c-0375-418d-b760-52387d51c433",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c45f3-4ca4-4e33-a985-0bf5d350f5c9",
   "metadata": {},
   "source": [
    "## Download Content\n",
    "In this example, we'll use a book titled [The Adventures of Sherlock Holmes](https://www.gutenberg.org/cache/epub/1661/pg1661.txt) as the source of the knowledge. \n",
    "This book made available for free by [Project Gutenberg](https://www.gutenberg.org). The book has a copyright status of public domain. For more information please refer to the detail [here](https://www.gutenberg.org/ebooks/1661)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de020162-6c5f-4e3a-b85c-d949fcb50d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_url = \"https://www.gutenberg.org/cache/epub/1661/pg1661.txt\" # The adventures of Sherlock Holmes\n",
    "data = urllib.request.urlopen(target_url)\n",
    "my_texts = []\n",
    "for line in data:\n",
    "    my_texts.append(line.decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edeedf6-fb49-4e00-89fb-af74775cecfa",
   "metadata": {},
   "source": [
    "Split the document into chunks and create question for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838dfed2-b73d-43b0-b9a5-b137fe05d975",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_size = 1000 # size of the document to determine number of batches\n",
    "batches = math.ceil(len(my_texts) / doc_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fed8f-7c85-47a6-9bf9-144247bb928d",
   "metadata": {},
   "source": [
    "## Automate Question Generation\n",
    "To evaluate a retriever system, we would first need a test set of questions on the documents. These questions need to be diverse, relevant, and coherent. Manually generating questions may be challenging because it first requires you to understand the documents, and spend lots of time coming up with questions for them.\n",
    "\n",
    "In the following step, we'll use a Bedrock model (e.g. Claude3 Sonnet) to help create questions from the given document chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cfdd9b-3a25-464f-ad92-2c1fc2e63bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(bedrock_runtime, model_id, documents):\n",
    "\n",
    "    prompt_template = \"\"\"The question should be diversed in nature \\\n",
    "across the document. The question should not contain options, not start with Q1/ Q2. \\\n",
    "Restrict the question to the context information provided.\\\n",
    "\n",
    "<document>\n",
    "{{document}}\n",
    "</document>\n",
    "\n",
    "\n",
    "Your response must follow the format as followed:\n",
    "\n",
    "Question: question\n",
    "Answer: answer\n",
    "\n",
    "Here are a few examples of the question and answer format:\n",
    "\n",
    "### Example\n",
    "Question: What does John likes to do when he's free?\n",
    "Answer: John like to read books and play soccer.\n",
    "\n",
    "### Example\n",
    "Question: When did Alice start her new role in company A?\n",
    "Answer: Alice started her new role last week. She's excited to get back to workforce after a long break. \n",
    "\n",
    "\n",
    "Think step by step and pay attention to the number of question to create. Only return the question and answer. Do not provide any other explanation or pretext.\n",
    "\n",
    "\"\"\"\n",
    "    system_prompt = \"\"\"You are a professor. Your task is to setup 1 question for an upcoming \\\n",
    "quiz/examination based on the given document wrapped in <document></document> XML tag.\"\"\"\n",
    "\n",
    "    prompt = prompt_template.replace(\"{{document}}\", documents)\n",
    "    temperature = 0.9\n",
    "    top_k = 250\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": temperature, \"maxTokens\": 512, \"topP\": 1.0}\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=[{\"text\": system_prompt}],\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    result = response['output']['message']['content'][0]['text']\n",
    "    q_pos = [(a.start(), a.end()) for a in list(re.finditer(\"Question:\", result))]\n",
    "    a_pos = [(a.start(), a.end()) for a in list(re.finditer(\"Answer:\", result))]\n",
    "\n",
    "    data_samples = {}\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for idx, q in enumerate(q_pos):\n",
    "        q_start = q[1]\n",
    "        a_start = a_pos[idx][0]\n",
    "        a_end = a_pos[idx][1]\n",
    "        question = result[q_start:a_start-1]\n",
    "        if idx == len(q_pos) - 1:\n",
    "            answer = result[a_end:]\n",
    "        else:\n",
    "            next_q_start = q_pos[idx+1][0]\n",
    "            answer = result[a_end:next_q_start-2]\n",
    "        print(f\"===============\")\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\")\n",
    "        questions.append(question.strip())\n",
    "        answers.append(answer.strip())\n",
    "    data_samples['question'] = questions\n",
    "    data_samples['ground_truth'] = answers\n",
    "    return data_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3882117-c84a-45e5-9279-427bc3c54ca4",
   "metadata": {},
   "source": [
    "Format the generated Q&A dataset into the following format:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"question\" : [...],\n",
    "  \"ground_truth\" : [ ... ]\n",
    "}\n",
    "``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad66da40-7922-43b0-ba57-e87906ba6ebd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "data_samples = {}\n",
    "data_samples['question'] = []\n",
    "data_samples['ground_truth'] = []\n",
    "for batch in range(batches-2): # skip the last 2 batches to avoid creating QA for content not directly related to the book.\n",
    "    batch_text_arr = my_texts[start:start+doc_size]\n",
    "    batch_text = \"\".join(batch_text_arr)\n",
    "    start += doc_size\n",
    "    ds = generate_questions(bedrock_runtime, model_id, batch_text)\n",
    "    data_samples['question'].extend(ds['question'])\n",
    "    data_samples['ground_truth'].extend(ds['ground_truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f37d85d-3013-42ee-9562-e88c7670566b",
   "metadata": {},
   "source": [
    "*Note:* After the QA pairs are generated, verify each question and answer to make sure there are missing missing information. If there are any empty question or answer, you should rerun the previous cell to regenerate the QA dataset. Missing question or answer will result in inconsistency in the RAG evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c94e88-a0a0-46b7-9a54-ed3e0a0a687d",
   "metadata": {},
   "source": [
    "# Save the Q&A Dataset\n",
    "In this final step, we'll save the Q&A output into a JSON file. This file will be used in the next [notebook](rag_evaluation.ipynb) which will focus on performing RAG evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ee0401-6189-4579-a70b-4747bd0b8d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/qa_samples.json\", \"w\") as f:\n",
    "    f.write(json.dumps(data_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca590e5-e8a9-4c46-a784-6af883ca3358",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

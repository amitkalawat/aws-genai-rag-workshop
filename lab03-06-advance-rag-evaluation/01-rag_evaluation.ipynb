{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142683cf-e687-4c50-bcbd-37d5256156cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-community langchain-core==0.2.10 langchain-aws ragas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d24c4ea",
   "metadata": {},
   "source": [
    "Restarting the Jupyterlab kernel to pick up the latest library installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837f4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb1a779-0000-4004-accf-959e1f79a33d",
   "metadata": {},
   "source": [
    "# RAG Evaluation\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has recently emerged as a promising approach in natural language processing. \n",
    "There are many real-world applications leveraging its capability to enhance generative models through the integration of external information retrieval. \n",
    "However, evaluating these RAG systems is not straightforward. There are unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. \n",
    "To address these challenges, we explore an evaluation and benchmarking of RAG systems call [RAGAS](https://docs.ragas.io/). \n",
    "Through this framework, we'll evaluate the effectiveness the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the RAG benchmarks, encompassing the possible output and ground truth pairs. \n",
    "\n",
    "To demonstrate RAG application evaluation capabilities, we'll build a RAG application using KonwledgeBases for Bedrock. This RAG application serves as a book assistant, using the Q&As created in the [00-qa_generator.ipynb](00-qa_generator.ipynb) as the basis for the evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f5ce5-c544-4d5f-b78c-76d9a037699a",
   "metadata": {},
   "source": [
    "Import library dependencies for the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71359081-39a3-47fd-b1ee-9e04fe177ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import urllib.request\n",
    "import sagemaker\n",
    "from datasets import Dataset\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "import re\n",
    "from utils import bedrock_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d668a5e-4ebb-4c8d-985f-ead9a810b422",
   "metadata": {},
   "source": [
    "## Download and Process Source Data\n",
    "In this example, we'll use a book titled [The Adventures of Sherlock Holmes](https://www.gutenberg.org/cache/epub/1661/pg1661.txt) as the source of the knowledge. \n",
    "This book made available for free by [Project Gutenberg](https://www.gutenberg.org). The book has a copyright status of public domain. For more information please refer to the detail [here](https://www.gutenberg.org/ebooks/1661)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f8e6ba-2b1b-4200-bbca-bb788c4386fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file_path = \"data/sherlock_holmes.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe5c26-48da-4174-b9c1-76abab531a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_url = \"https://www.gutenberg.org/cache/epub/1661/pg1661.txt\" # The adventures of Sherlock Holmes\n",
    "urllib.request.urlretrieve(target_url, download_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadb9cc-52cb-4393-832a-ab20916bebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(download_file_path, \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15693aa0-d8d2-43a0-ae2a-a39422922ed7",
   "metadata": {},
   "source": [
    "To create the knowledge source, we are going to extract the chapters from the book, and save each into a separate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b034137-2534-440f-9cf0-6c1e1b58cea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_chapter_end= 0\n",
    "chapter_contents = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6380144-98cf-49ab-804b-80341fe4652f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chapter_separator = \"\\n\\n\\n\\n\" # The chapter separator used to extract the chapter content.\n",
    "while True:\n",
    "    chapter_end = re.search(chapter_separator, data[prev_chapter_end:]) #use regex to find the end of each chapter.\n",
    "    if chapter_end:\n",
    "        chapter_end_start_pos = chapter_end.start()\n",
    "        chapter_content = data[prev_chapter_end:prev_chapter_end+chapter_end_start_pos]\n",
    "        prev_chapter_end = prev_chapter_end + chapter_end.end()\n",
    "        chapter_contents.append(chapter_content)\n",
    "    else:\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbd2b8-9aee-42b5-b7c2-981d1ccbf0f3",
   "metadata": {},
   "source": [
    "Upload the book to S3 bucket so that it could be used by Knowledge Bases for Bedrock as the source for the RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120305b-1ecb-4494-a66f-e0bb794a84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use boto3 s3 to upload a string to S3 \n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = \"bedrock/knowledgebase/datasources/sherlock_holmes\"\n",
    "\n",
    "start = 0\n",
    "s3 = boto3.client(\"s3\")\n",
    "for idx, chapter_content in enumerate(chapter_contents[2:]):\n",
    "    s3.put_object(\n",
    "        Body=chapter_content,\n",
    "        Bucket=default_bucket,\n",
    "        Key=f\"{s3_prefix}/chapter_{idx+1}.txt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5520229-634c-4daa-8a9c-3580dea05875",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In our setup, we'll be using a Bedrock Claude models as the LLM. In addition, we'll use the Amazon Titan Text Embedding v2 to convert the documents into embeddings and store the vectors into Opensearch serverless collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b187a-f5e6-43ae-80ee-5a4c9e71ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_role = sagemaker.get_execution_role()\n",
    "bedrock = boto3.client(\"bedrock\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "agent_runtime = boto3.client('bedrock-agent-runtime')\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\")\n",
    "model_arn = \"arn:aws:bedrock:us-west-2::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "evaluation_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\" # model ID to be used with RAGAS for evaluation.\n",
    "embedding_dim = 1024\n",
    "region = bedrock.meta.region_name\n",
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\" # model ID for the embedding model to be used by Knowledge Bases for Bedrock\n",
    "embedding_model_arn = f\"arn:aws:bedrock:{region}::foundation-model/amazon.titan-embed-text-v2:0\" # model arn to be used for RAGAS evalaution.\n",
    "boto3_credentials = boto3.Session().get_credentials() # creadentials for opensearch cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927857aa-09a3-4a88-91ca-c0731248c0d4",
   "metadata": {},
   "source": [
    "# Create a Knowledge Base using Amazon Bedrock\n",
    "\n",
    "The following section describes the steps to take in order to create a knowledge base in Bedrock. We are going to use the Amazon Bedrock Agent SDK and Opensearch SDK to create the required components.\n",
    "\n",
    "## How it works\n",
    "Knowledge base for Amazon Bedrock help you take advantage of Retrieval Augmented Generation (RAG), a popular technique that involves drawing information from a data store to augment the responses generated by Large Language Models (LLMs). With this approach, your application can query the knowledge base to return most relevant information found in your knowledge base to answer the query either with direct quotations from sources or with natural responses generated from the query results.\n",
    "\n",
    "There are 2 main processes involved in carrying out RAG functionality via Knowledge Bases for Bedrock:\n",
    "\n",
    "1. Pre-processing - Ingest source data, create embeddings for the data and populate the embeddings into a vector database.\n",
    "2. Runtime Execution - Query the vectorDB for similar documents based on user query and return topk documents as the basis for the LLM to provide a response. The following diagrams illustrate schematically how RAG is carried out. Knowledge base simplifies the setup and implementation of RAG by automating several steps in this process.\n",
    "\n",
    "### Preprocesing Stage\n",
    "![RAG preprocessing](img/br-kb-preprod-diagram.png)\n",
    "\n",
    "### Execution Stage\n",
    "![RAG execution](img/br-kb-runtime-diagram.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc235767-81cd-478b-a566-dedc249bade9",
   "metadata": {},
   "source": [
    "Define variables to use for creating a knowledge bases for Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01c7d8-896a-4126-a1d1-32f3732bd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = str(uuid.uuid4().hex)[:5]\n",
    "index_name = f\"bedrock-kb-{random_id}\"\n",
    "knowledge_base_name = f\"bedrock-kb-{random_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9d7e4-e900-4a77-a76e-69dd8a88ee29",
   "metadata": {},
   "source": [
    "## Steps for creating a Knowledge Base for Bedrock application\n",
    "Creating a knowledge base involves the following steps:\n",
    "\n",
    "* Create an opensearch serverless collection as the vector DB.\n",
    "* Create an index for the collection to be used for all the documents\n",
    "* Create the required IAM service roles for Bedrock to integrate with the collection\n",
    "* Create a Knowledge Base for Bedrock application.\n",
    "* Create a data ingestion job to create the embeddings into the opensearch serverless collection.\n",
    "\n",
    "Luckily, all the steps outlined above are provided as a helper function so you don't have to do this yourself!\n",
    "\n",
    "**Note:** The knowledge base creation step below takes about 5 minutes. Please be patient and and let it finish everything before stopping any processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2195634",
   "metadata": {},
   "source": [
    "Restores the environment variables from lab setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603973ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3fcea5-6f3e-4a7b-9ca4-5057e721829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_id = bedrock_helper.create_knowledge_base(knowledge_base_name=knowledge_base_name, \n",
    "                               bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn, \n",
    "                               embedding_model_arn=embedding_model_arn, \n",
    "                               embedding_dim=embedding_dim, \n",
    "                               s3_bucket=default_bucket, \n",
    "                               s3_prefix=s3_prefix, \n",
    "                               oss_host=vector_host, \n",
    "                               oss_collection_id=vector_collection_id, \n",
    "                               oss_collection_arn=vector_collection_arn, \n",
    "                               index_name=index_name, \n",
    "                               region=region, \n",
    "                               credentials=boto3_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16771918-74b8-4011-939a-93770b1303bd",
   "metadata": {},
   "source": [
    "## Auto Generate Questions From the Documents Using LLM\n",
    "We have prepared a list of questions and answers from the book which we'll used as the base for the questions and answers. These questions and answers are generated by an LLM in Bedrock.\n",
    "\n",
    "Important: Just as many LLM applications, it's important to leverage human in the loop to validate the Q&A generated by the LLM to ensure they are correct and accurate. For our experiment, all the questions and answers have been validated by human, so that we could use them as the ground truth for a fair model and RAG evaluation process.\n",
    "\n",
    "The Q&A data serves as the foundation for the RAG evaluation based on the approaches that we are going to implement. We'll define the generated answers from this step as ground truth data.\n",
    "\n",
    "Next, based on the generated questions, we'll use Bedrock Agent SDK to retrieve the contexts that's most relevant to the question, and generate answers for each one of them. These data would be served as the source data for standard RAG approach.\n",
    "\n",
    "We also share a notebook that walks through the process of using an LLM to generate questions and answers [here](00-qa_generator.ipynb).\n",
    "\n",
    "The QA dataset is formatted as JSON defined as followed:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"question\" : [ ... ],\n",
    "  \"ground_truth\" : [ ... ]\n",
    "}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f204f-14d0-443d-824e-bec497ba35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/qa_samples.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    data_samples = json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035e032-d728-418b-824b-447ffeb56bfd",
   "metadata": {},
   "source": [
    "## Invoke Knowledge Bases For Bedrock \n",
    "In the following step, we'll iterate each generated question from the Q&A data as query to invoke Knowledge bases using the relevant [boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) API call. The knowledge base  generates the response with the results and corresponding contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb4adf-20cf-4a40-8ba5-d7e5a21a96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "model_responses = []\n",
    "for q in data_samples['question']:\n",
    "    local_contexts = []\n",
    "    response = agent_runtime.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': q\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'generationConfiguration': {\n",
    "                    'inferenceConfig':  {\n",
    "                        'textInferenceConfig': {\n",
    "                            'stopSequences': [\n",
    "                                'Human:',\n",
    "                            ],\n",
    "                            'temperature': 0.1,\n",
    "                            'topP': 0.9\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'knowledgeBaseId': knowledge_base_id,\n",
    "                'modelArn':  model_arn\n",
    "            },\n",
    "            'type': 'KNOWLEDGE_BASE'\n",
    "        }\n",
    "    )\n",
    "    model_response = response['citations'][0]['generatedResponsePart']['textResponsePart']['text']\n",
    "    model_responses.append(model_response)\n",
    "    for retrievedReference in response['citations'][0]['retrievedReferences']:\n",
    "        context = retrievedReference['content']['text']\n",
    "        local_contexts.append(context)\n",
    "    contexts.append(local_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4712b3-b70f-42bb-997e-9adfdee40a39",
   "metadata": {},
   "source": [
    "## Combining Dataset For RAGAS Evaluation\n",
    "Now that we have the model responses and the contexts, we'll use these information to combine with QA dataset to build the dataset required to use RAGAS evaluation framework:\n",
    "\n",
    "1. Questions \n",
    "2. Context for the RAG\n",
    "3. Response from the model\n",
    "4. Ground truths data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ed091-282e-4d16-9a68-cbb8585a45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples['contexts'] = contexts\n",
    "data_samples['answer'] = model_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdee5e9-e5b4-4509-bd9a-1f9cb59f128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5526aa-8b7e-49d2-b5c5-4eb649771e0b",
   "metadata": {},
   "source": [
    "## RAG Evaluation using RAGAS Framework\n",
    "To evaluate the effectiveness of RAG, well use a framework called [RAGAS](https://github.com/explodinggradients/ragas). \n",
    "The framework provides a suite of metrics which can be used to evaluate different dimensions. \n",
    "\n",
    "At a high level, RAGAS evaluation focuses on the following key components:\n",
    "\n",
    "![RAGAS evaluation](img/ragas-evaluations.png)\n",
    "\n",
    "Here's are the summary of some of the evaluation components supported in RAGAS:\n",
    "\n",
    "### Faithfullness\n",
    "This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better. The generated answer is regarded as faithful if all the claims that are made in the answer can be inferred from the given context. To calculate this a set of claims from the generated answer is first identified. Then each one of these claims are cross checked with given context to determine if it can be inferred from given context or not.\n",
    "\n",
    "### Answer Correctness\n",
    "The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. These aspects are combined using a weighted scheme to formulate the answer correctness score. Users also have the option to employ a ‘threshold’ value to round the resulting score to binary, if desired.\n",
    "\n",
    "### Answer Relevancy\n",
    "The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information. This metric is computed using the question and the answer, with values ranging between 0 and 1, where higher scores indicate better relevancy.\n",
    "\n",
    "### Answer Similarity\n",
    "The concept of Answer Semantic Similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment between the generated answer and the ground truth.\n",
    "\n",
    "### Context Precision\n",
    "Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the question, ground_truth and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.\n",
    "\n",
    "\n",
    "### Context Recall \n",
    "Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "\n",
    "To estimate context recall from the ground truth answer, each claim in the ground truth answer is analyzed to determine whether it can be attributed to the retrieved context or not. In an ideal scenario, all claims in the ground truth answer should be attributable to the retrieved context.\n",
    "\n",
    "\n",
    "In our example, we'll explore the following evaluation components:\n",
    "\n",
    "* Faithfulness\n",
    "* Answer Correctness\n",
    "* Answer Similarity\n",
    "* Answer Relevancy\n",
    "* Context Precision\n",
    "* Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2a311-0b49-4cf3-b703-0cd8f87625ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    answer_similarity,\n",
    "    answer_relevancy,\n",
    "    context_precision, \n",
    "    context_recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a7bd8-6c7f-4ed5-9f62-acc62f33648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio \n",
    "nest_asyncio.apply() # Based on Ragas documenttion this is only needed when running in a jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ebcfe-bb52-4ad3-b3e7-34e8167d46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    answer_similarity,\n",
    "    answer_relevancy,\n",
    "    context_precision, \n",
    "    context_recall\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba79101-827f-4166-befb-04bf55a7dd04",
   "metadata": {},
   "source": [
    "## Setup Bedrock model configurations\n",
    "RAGAS fully integrates Bedrock foundation models and embedding models into their framework. In this lab, we'll use a more powerful foundation model [Claude3 Sonnet](https://aws.amazon.com/bedrock/claude/) to help evaluate the RAG application. Since RAGAS works directly with [langchain](https://www.langchain.com/), an open source framework for building LLM applications, we'll define Bedrock models via langchain framework, and pass these model objects to RAGAS for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ffbfe-3b56-465f-a773-6a436c01b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"region_name\": region,  # E.g. \"us-east-1\"\n",
    "    \"model_id\": evaluation_model_id,  # E.g \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    \"model_kwargs\": {\"temperature\": 0.9},\n",
    "}\n",
    "\n",
    "bedrock_model = ChatBedrock(\n",
    "    region_name=config[\"region_name\"],\n",
    "    model_id=config[\"model_id\"],\n",
    "    model_kwargs=config[\"model_kwargs\"],\n",
    ")\n",
    "\n",
    "# init the embeddings\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    region_name=config[\"region_name\"],\n",
    "    model_id = embedding_model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78100e7-d981-4ba3-874b-5a486bb07b62",
   "metadata": {},
   "source": [
    "Invoke RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf50407-d98c-46a1-bbb8-d4a47dabff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result = evaluate(\n",
    "    ds,\n",
    "    metrics=metrics,\n",
    "    llm=bedrock_model,\n",
    "    embeddings=bedrock_embeddings,\n",
    "    raise_exceptions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69c865-126b-4707-b8ba-2f9744bc7aa1",
   "metadata": {},
   "source": [
    "Shows the evaluation results for each entry via pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c55b74-7d05-4803-ac0a-13d58b090b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluation_result.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908f52b-af93-413a-8de2-573f63c89665",
   "metadata": {},
   "source": [
    "## RAG Evaluation Summary\n",
    "Finally, we'll use RAGAS evaluation to help provide an average score for each evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3474f-66eb-4b1e-a5da-bb6fd2250d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"A summarized report for standard RAG approach based on RAGAS evaluation: \n",
    "faithfulness: {evaluation_result['faithfulness']}\n",
    "answer_correctness: {evaluation_result['answer_correctness']}\n",
    "answer_similarity: {evaluation_result['answer_similarity']}\n",
    "answer_relevancy: {evaluation_result['answer_relevancy']}\n",
    "context_precision: {evaluation_result['context_precision']}\n",
    "context_recall: {evaluation_result['context_recall']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0d8f8-5829-417a-8034-2acd3ca9ece2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we demonstrates how to evaluate a RAG application using Amazon Bedrock and RAGAS framework. We started by uploaded the sample texts into an S3 bucket for creating the corresponding vector embeddings. After the data is uploaded to S3, we created a knowledge Base for Bedrock application and integrated it with an OpenSearch serverless collection. We fired a data ingestion job using the Bedrock Agent SDK to create the vector embeddings for the data on S3, and persists the vectors into the given Opensearch serverless collection.\n",
    "\n",
    "To perform RAG evaluation on both the standard RAG and the two stage retrieval with a reranking model approach, we used an open source framework RAGAS focusing on faithfullness, answer correctness,, answer similarity, answer_relevancy, context_precision and context_racall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af527e6b-4f78-4234-9c78-4d88490bec28",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "If you are done with the experiment, you can delete the resources used in this notebook by running the following cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a27267-096f-42bb-a4f6-c7a05fb743e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.delete_knowledge_base(\n",
    "    knowledgeBaseId=knowledge_base_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a85ddf-0c5e-4b64-9526-e936a3692fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

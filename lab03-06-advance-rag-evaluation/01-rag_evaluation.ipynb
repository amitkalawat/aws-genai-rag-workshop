{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb1a779-0000-4004-accf-959e1f79a33d",
   "metadata": {},
   "source": [
    "# RAG Evaluation\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) has recently emerged as a promising approach in natural language processing. \n",
    "There are many real-world applications leveraging its capability to enhance generative models through the integration of external information retrieval. \n",
    "However, evaluating these RAG systems is not straightforward. There are unique challenges due to their hybrid structure and reliance on dynamic knowledge sources. \n",
    "To address these challenges, we explore an evaluation and benchmarking of RAG systems call [RAGAS](https://docs.ragas.io/). \n",
    "Through this framework, we'll evaluate the effectiveness the Retrieval and Generation components, such as relevance, accuracy, and faithfulness, within the RAG benchmarks, encompassing the possible output and ground truth pairs. \n",
    "\n",
    "To demonstrate RAG application evaluation capabilities, we'll build a RAG application using KonwledgeBases for Bedrock. This RAG application serves as a book assistant, using the Q&As created in the [00-qa_generator.ipynb](00-qa_generator.ipynb) as the basis for the evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4f5ce5-c544-4d5f-b78c-76d9a037699a",
   "metadata": {},
   "source": [
    "Import library dependencies for the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71359081-39a3-47fd-b1ee-9e04fe177ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "import urllib.request\n",
    "import sagemaker\n",
    "from datasets import Dataset\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_community.embeddings import BedrockEmbeddings\n",
    "import re\n",
    "from utils import bedrock_helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca28c5f-6dac-475a-af4a-60815586dc0e",
   "metadata": {},
   "source": [
    "Restore variables from the setup for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33b2834-d05a-4e4c-8448-2ea993d0426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dbd2b8-9aee-42b5-b7c2-981d1ccbf0f3",
   "metadata": {},
   "source": [
    "## Upload Source Data to S3\n",
    "In this example, we'll use a book titled [The Adventures of Sherlock Holmes](https://www.gutenberg.org/cache/epub/1661/pg1661.txt) as the source of the knowledge.  This book made available for free by [Project Gutenberg](https://www.gutenberg.org). The book has a copyright status of public domain. For more information please refer to the detail [here](https://www.gutenberg.org/ebooks/1661).\n",
    "\n",
    "Based on the previous notebook, we extracted the book content into chapters. In this notebook we'll upload each chapter to S3 bucket so that it could be used by Knowledge Bases for Bedrock as the source for the RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3120305b-1ecb-4494-a66f-e0bb794a84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use boto3 s3 to upload a string to S3 \n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "s3_prefix = \"bedrock/knowledgebase/datasources/sherlock_holmes\"\n",
    "\n",
    "start = 0\n",
    "s3 = boto3.client(\"s3\")\n",
    "for idx, chapter_content in enumerate(chapter_contents[2:]):\n",
    "    s3.put_object(\n",
    "        Body=chapter_content,\n",
    "        Bucket=default_bucket,\n",
    "        Key=f\"{s3_prefix}/chapter_{idx+1}.txt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5520229-634c-4daa-8a9c-3580dea05875",
   "metadata": {},
   "source": [
    "## Setup\n",
    "In our setup, we'll be using a Bedrock Claude models as the LLM. In addition, we'll use the Amazon Titan Text Embedding v2 to convert the documents into embeddings and store the vectors into Opensearch serverless collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b187a-f5e6-43ae-80ee-5a4c9e71ca5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_role = sagemaker.get_execution_role()\n",
    "bedrock = boto3.client(\"bedrock\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "agent_runtime = boto3.client('bedrock-agent-runtime')\n",
    "bedrock_agent = boto3.client(\"bedrock-agent\")\n",
    "evaluation_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\" # model ID to be used with RAGAS for evaluation.\n",
    "embedding_dim = 1024\n",
    "region = bedrock.meta.region_name\n",
    "model_arn = f\"arn:aws:bedrock:{region}::foundation-model/anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "embedding_model_id = \"amazon.titan-embed-text-v2:0\" # model ID for the embedding model to be used by Knowledge Bases for Bedrock\n",
    "embedding_model_arn = f\"arn:aws:bedrock:{region}::foundation-model/amazon.titan-embed-text-v2:0\" # model arn to be used for RAGAS evalaution.\n",
    "boto3_credentials = boto3.Session().get_credentials() # creadentials for opensearch cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927857aa-09a3-4a88-91ca-c0731248c0d4",
   "metadata": {},
   "source": [
    "# Create a Knowledge Base using Amazon Bedrock\n",
    "\n",
    "The following section describes the steps to take in order to create a knowledge base in Bedrock. We are going to use the Amazon Bedrock Agent SDK and Opensearch SDK to create the required components.\n",
    "\n",
    "## How it works\n",
    "Knowledge base for Amazon Bedrock help you take advantage of Retrieval Augmented Generation (RAG), a popular technique that involves drawing information from a data store to augment the responses generated by Large Language Models (LLMs). With this approach, your application can query the knowledge base to return most relevant information found in your knowledge base to answer the query either with direct quotations from sources or with natural responses generated from the query results.\n",
    "\n",
    "There are 2 main processes involved in carrying out RAG functionality via Knowledge Bases for Bedrock:\n",
    "\n",
    "1. Pre-processing - Ingest source data, create embeddings for the data and populate the embeddings into a vector database.\n",
    "2. Runtime Execution - Query the vectorDB for similar documents based on user query and return topk documents as the basis for the LLM to provide a response. The following diagrams illustrate schematically how RAG is carried out. Knowledge base simplifies the setup and implementation of RAG by automating several steps in this process.\n",
    "\n",
    "### Preprocesing Stage\n",
    "![RAG preprocessing](img/br-kb-preprod-diagram.png)\n",
    "\n",
    "### Execution Stage\n",
    "![RAG execution](img/br-kb-runtime-diagram.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc235767-81cd-478b-a566-dedc249bade9",
   "metadata": {},
   "source": [
    "Define variables to use for creating a knowledge bases for Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01c7d8-896a-4126-a1d1-32f3732bd8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_id = str(uuid.uuid4().hex)[:5]\n",
    "index_name = f\"bedrock-kb-{random_id}\"\n",
    "knowledge_base_name = f\"bedrock-kb-{random_id}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f9d7e4-e900-4a77-a76e-69dd8a88ee29",
   "metadata": {},
   "source": [
    "## Steps for creating a Knowledge Base for Bedrock application\n",
    "Creating a knowledge base involves the following steps:\n",
    "\n",
    "* Create an opensearch serverless collection as the vector DB.\n",
    "* Create an index for the collection to be used for all the documents\n",
    "* Create the required IAM service roles for Bedrock to integrate with the collection\n",
    "* Create a Knowledge Base for Bedrock application.\n",
    "* Create a data ingestion job to create the embeddings into the opensearch serverless collection.\n",
    "\n",
    "Luckily, all the steps outlined above are provided as a helper function so you don't have to do this yourself!\n",
    "\n",
    "**Note:** The knowledge base creation step below takes about 5 minutes. Please be patient and and let it finish everything before stopping any processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2195634",
   "metadata": {},
   "source": [
    "Restores the environment variables from lab setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3fcea5-6f3e-4a7b-9ca4-5057e721829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_id = bedrock_helper.create_knowledge_base(knowledge_base_name=knowledge_base_name, \n",
    "                               bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn, \n",
    "                               embedding_model_arn=embedding_model_arn, \n",
    "                               embedding_dim=embedding_dim, \n",
    "                               s3_bucket=default_bucket, \n",
    "                               s3_prefix=s3_prefix, \n",
    "                               oss_host=vector_host, \n",
    "                               oss_collection_id=vector_collection_id, \n",
    "                               oss_collection_arn=vector_collection_arn, \n",
    "                               index_name=index_name, \n",
    "                               region=region, \n",
    "                               credentials=boto3_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16771918-74b8-4011-939a-93770b1303bd",
   "metadata": {},
   "source": [
    "## Auto Generate Questions From the Documents Using LLM\n",
    "We have prepared a list of questions and answers from the book which we'll used as the base for the questions and answers. These questions and answers are generated by an LLM in Bedrock.\n",
    "\n",
    "Important: Just as many LLM applications, it's important to leverage human in the loop to validate the Q&A generated by the LLM to ensure they are correct and accurate. For our experiment, all the questions and answers have been validated by human, so that we could use them as the ground truth for a fair model and RAG evaluation process.\n",
    "\n",
    "The Q&A data serves as the foundation for the RAG evaluation based on the approaches that we are going to implement. We'll define the generated answers from this step as ground truth data.\n",
    "\n",
    "Next, based on the generated questions, we'll use Bedrock Agent SDK to retrieve the contexts that's most relevant to the question, and generate answers for each one of them. These data would be served as the source data for standard RAG approach.\n",
    "\n",
    "We also share a notebook that walks through the process of using an LLM to generate questions and answers [here](00-qa_generator.ipynb).\n",
    "\n",
    "The QA dataset is formatted as JSON defined as followed:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"question\" : [ ... ],\n",
    "  \"ground_truth\" : [ ... ]\n",
    "}\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f204f-14d0-443d-824e-bec497ba35ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/qa_samples.json\", \"r\") as f:\n",
    "    data = f.read()\n",
    "    data_samples = json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035e032-d728-418b-824b-447ffeb56bfd",
   "metadata": {},
   "source": [
    "## Invoke Knowledge Bases For Bedrock \n",
    "In the following step, we'll iterate each generated question from the Q&A data as query to invoke Knowledge bases using the relevant [boto3 SDK](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) API call. The knowledge base  generates the response with the results and corresponding contexts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb4adf-20cf-4a40-8ba5-d7e5a21a96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = []\n",
    "model_responses = []\n",
    "for q in data_samples['question']:\n",
    "    local_contexts = []\n",
    "    response = agent_runtime.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': q\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'generationConfiguration': {\n",
    "                    'inferenceConfig':  {\n",
    "                        'textInferenceConfig': {\n",
    "                            'stopSequences': [\n",
    "                                'Human:',\n",
    "                            ],\n",
    "                            'temperature': 0.1,\n",
    "                            'topP': 0.9\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                'knowledgeBaseId': knowledge_base_id,\n",
    "                'modelArn':  model_arn\n",
    "            },\n",
    "            'type': 'KNOWLEDGE_BASE'\n",
    "        }\n",
    "    )\n",
    "    model_response = response['citations'][0]['generatedResponsePart']['textResponsePart']['text']\n",
    "    model_responses.append(model_response)\n",
    "    for retrievedReference in response['citations'][0]['retrievedReferences']:\n",
    "        context = retrievedReference['content']['text']\n",
    "        local_contexts.append(context)\n",
    "    contexts.append(local_contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4712b3-b70f-42bb-997e-9adfdee40a39",
   "metadata": {},
   "source": [
    "## Combining Dataset For RAGAS Evaluation\n",
    "Now that we have the model responses and the contexts, we'll use these information to combine with QA dataset to build the dataset required to use RAGAS evaluation framework:\n",
    "\n",
    "1. Questions \n",
    "2. Context for the RAG\n",
    "3. Response from the model\n",
    "4. Ground truths data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73ed091-282e-4d16-9a68-cbb8585a45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples['contexts'] = contexts\n",
    "data_samples['answer'] = model_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdee5e9-e5b4-4509-bd9a-1f9cb59f128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_dict(data_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5526aa-8b7e-49d2-b5c5-4eb649771e0b",
   "metadata": {},
   "source": [
    "## RAG Evaluation using RAGAS Framework\n",
    "To evaluate the effectiveness of RAG, well use a framework called [RAGAS](https://github.com/explodinggradients/ragas). \n",
    "The framework provides a suite of metrics which can be used to evaluate different dimensions. \n",
    "\n",
    "At a high level, RAGAS evaluation focuses on the following key components:\n",
    "\n",
    "![RAGAS evaluation](img/ragas-evaluations.png)\n",
    "\n",
    "Here's are the summary of some of the evaluation components supported in RAGAS:\n",
    "\n",
    "### Faithfullness\n",
    "This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better. The generated answer is regarded as faithful if all the claims that are made in the answer can be inferred from the given context. To calculate this a set of claims from the generated answer is first identified. Then each one of these claims are cross checked with given context to determine if it can be inferred from given context or not.\n",
    "\n",
    "### Answer Correctness\n",
    "The assessment of Answer Correctness involves gauging the accuracy of the generated answer when compared to the ground truth. This evaluation relies on the ground truth and the answer, with scores ranging from 0 to 1. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. Answer correctness encompasses two critical aspects: semantic similarity between the generated answer and the ground truth, as well as factual similarity. These aspects are combined using a weighted scheme to formulate the answer correctness score. Users also have the option to employ a ‘threshold’ value to round the resulting score to binary, if desired.\n",
    "\n",
    "### Answer Relevancy\n",
    "The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information. This metric is computed using the question and the answer, with values ranging between 0 and 1, where higher scores indicate better relevancy.\n",
    "\n",
    "### Answer Similarity\n",
    "The concept of Answer Semantic Similarity pertains to the assessment of the semantic resemblance between the generated answer and the ground truth. This evaluation is based on the ground truth and the answer, with values falling within the range of 0 to 1. A higher score signifies a better alignment between the generated answer and the ground truth.\n",
    "\n",
    "### Context Precision\n",
    "Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally all the relevant chunks must appear at the top ranks. This metric is computed using the question, ground_truth and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.\n",
    "\n",
    "\n",
    "### Context Recall \n",
    "Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed based on the ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.\n",
    "\n",
    "To estimate context recall from the ground truth answer, each claim in the ground truth answer is analyzed to determine whether it can be attributed to the retrieved context or not. In an ideal scenario, all claims in the ground truth answer should be attributable to the retrieved context.\n",
    "\n",
    "\n",
    "In our example, we'll explore the following evaluation components:\n",
    "\n",
    "* Faithfulness\n",
    "* Answer Correctness\n",
    "* Answer Similarity\n",
    "* Answer Relevancy\n",
    "* Context Precision\n",
    "* Context Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a2a311-0b49-4cf3-b703-0cd8f87625ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    answer_similarity,\n",
    "    answer_relevancy,\n",
    "    context_precision, \n",
    "    context_recall\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a7bd8-6c7f-4ed5-9f62-acc62f33648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio \n",
    "nest_asyncio.apply() # Based on Ragas documenttion this is only needed when running in a jupyter notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ebcfe-bb52-4ad3-b3e7-34e8167d46d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    answer_similarity,\n",
    "    answer_relevancy,\n",
    "    context_precision, \n",
    "    context_recall\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba79101-827f-4166-befb-04bf55a7dd04",
   "metadata": {},
   "source": [
    "## Setup Bedrock model configurations\n",
    "RAGAS fully integrates Bedrock foundation models and embedding models into their framework. In this lab, we'll use a more powerful foundation model [Claude3 Sonnet](https://aws.amazon.com/bedrock/claude/) to help evaluate the RAG application. Since RAGAS works directly with [langchain](https://www.langchain.com/), an open source framework for building LLM applications, we'll define Bedrock models via langchain framework, and pass these model objects to RAGAS for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657ffbfe-3b56-465f-a773-6a436c01b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"region_name\": region,  # E.g. \"us-east-1\"\n",
    "    \"model_id\": evaluation_model_id,  # E.g \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "    \"model_kwargs\": {\"temperature\": 0.9},\n",
    "}\n",
    "\n",
    "bedrock_model = ChatBedrock(\n",
    "    region_name=config[\"region_name\"],\n",
    "    model_id=config[\"model_id\"],\n",
    "    model_kwargs=config[\"model_kwargs\"],\n",
    ")\n",
    "\n",
    "# init the embeddings\n",
    "bedrock_embeddings = BedrockEmbeddings(\n",
    "    region_name=config[\"region_name\"],\n",
    "    model_id = embedding_model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78100e7-d981-4ba3-874b-5a486bb07b62",
   "metadata": {},
   "source": [
    "Invoke RAGAS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf50407-d98c-46a1-bbb8-d4a47dabff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result = evaluate(\n",
    "    ds,\n",
    "    metrics=metrics,\n",
    "    llm=bedrock_model,\n",
    "    embeddings=bedrock_embeddings,\n",
    "    raise_exceptions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf69c865-126b-4707-b8ba-2f9744bc7aa1",
   "metadata": {},
   "source": [
    "Shows the evaluation results for each entry via pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c55b74-7d05-4803-ac0a-13d58b090b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = evaluation_result.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2908f52b-af93-413a-8de2-573f63c89665",
   "metadata": {},
   "source": [
    "## RAG Evaluation Summary\n",
    "Finally, we'll use RAGAS evaluation to help provide an average score for each evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3474f-66eb-4b1e-a5da-bb6fd2250d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"A summarized report for standard RAG approach based on RAGAS evaluation: \n",
    "faithfulness: {evaluation_result['faithfulness']}\n",
    "answer_correctness: {evaluation_result['answer_correctness']}\n",
    "answer_similarity: {evaluation_result['answer_similarity']}\n",
    "answer_relevancy: {evaluation_result['answer_relevancy']}\n",
    "context_precision: {evaluation_result['context_precision']}\n",
    "context_recall: {evaluation_result['context_recall']}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af86c6bb-f8af-4b04-9b02-c2d4dfa16e9d",
   "metadata": {},
   "source": [
    "# RAG Evaluation Analysis and Recommendations\n",
    "Proper RAG evaluation helps identify potential weaknesses in both the retrieval and generation components, allowing for targeted improvements. It also helps in assessing the model's ability to provide up-to-date, factual information, which is especially important in rapidly evolving fields or for time-sensitive applications. Furthermore, RAG evaluation plays a vital role in mitigating hallucinations or false information generation, a common concern with large language models. By rigorously testing and evaluating RAG systems, developers can enhance the reliability, accuracy, and trustworthiness of the generative AI application.\n",
    "\n",
    "As mentioned in the diagram above, RAGAS framework focuses on evaluation in the Retrieval and Generation workflows. Let's look into those in more detail based on the metrics captured in the evaluation process:\n",
    "\n",
    "## Retrieval Evaluation Metrics\n",
    "1. **Context Precision** is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. As a result, a high context precision suggests the relevant contexts returned from the retrieval are effective. A low context precision could indicate the quality of the data is not good enough to provide relevant context. One recommendation is to revisit the chunking strategy and use a different chunking approach for the documents. Another suggestion is to consider using a more effective embedding model to better capture the semantics of the documents. Please refer to the [documentation](https://docs.ragas.io/en/latest/concepts/metrics/context_precision.html) for more information about context precision.\n",
    "   \n",
    "2. **Context Recall** measures the extent to which the retrieved context aligns with the ground truth. A high context recall suggests the retrieved contexts reflect accurately the ground truth data. A low context recall indicates the retrieved context might not accurately reflect the ground truth information. One recommendation is to revisit the chunking strategy for parsing the documents. Another suggestion is to revisit the documents to ensure the information in the ground truth are well presented within the documents. If these do not address the issue, consider a more effective embedding model to better capture the semantics of the documents. Please refer to the [documentation](https://docs.ragas.io/en/latest/concepts/metrics/context_recall.html) for more information about context recall.\n",
    "\n",
    "## Generation Evaluation Metrics\n",
    "1. **Faithfulness** measures the consistency of the generated answer against the given context. In other words, faithfulness measures the severity of hallucinations in the model response. There are 2 considerations when analyzing the faithfulness metric. First, we determine whether the retrieved contexts are good enough to provide the evidence for the LLM to generate an answer. This can be done using the retrieval metrics, such as context recall or context precision. If the retrieval metrics are high, then a low faithfulness would indicate a weakness in the LLM in generating the answers from the given context. Please refer to the [documentation](https://docs.ragas.io/en/latest/concepts/metrics/faithfulness.html) for more information about faithfulness metric. \n",
    "\n",
    "2. **Answer Correctness** measures the accuracy of the generated answer against the ground truth data. A higher score indicates a closer alignment between the generated answer and the ground truth, signifying better correctness. A low answer correctness score could indicate the LLM being capable of providing an answer that completely satisfy the question. This metric could be influenced by the quality of the retrieved contexts, therefore it should be considered along with the retrieval metrics to provide a complete evaluation. Please refer to this [documentation](https://docs.ragas.io/en/latest/concepts/metrics/answer_correctness.html) for more information about answer correctness.\n",
    "\n",
    "3. **Answer Similarity** measures the semantic resemblance between the generated answer and the ground truth. It uses cosine similarity to calculate the similarity score between the ground truth and the answer. A high similarity score indicates a strong alignment between the generated answer and the corresponding ground truth data. A low similarity score could indicate incompleteness in the generated answer relative to the ground truth. This metric could be influenced by the quality of the retrieved contexts. If the context precision or context recall are low, there is a high chance the answer similarity will also be low. In that case, the problem is probably lies in the retrieval process. Additionally, if the faithfulness score is low, the answer similarity will also be impacted. In this situation, addressing faithfulness score should help improving anwer similarity. Please refer to this [documentation](https://docs.ragas.io/en/latest/concepts/metrics/semantic_similarity.html) for more information about answer similarity.\n",
    "   \n",
    "4. **Answer relevancy** focuses on assessing how relevant the generated answer is to the question. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. Similar to answer similarity, this metric should be analyzed together with the retrieval metrics. Keep in mind that since this metric does not rely on the ground truth data, it could potentially produce a higher score, even though the generated answer is not completely accurate. The recommendation is to use **answer similarity** score together to gain a better insights into the effectiveness of the RAG workflow. Please refer to this [documentation](https://docs.ragas.io/en/latest/concepts/metrics/answer_relevance.html) for more information about answer relevancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe0d8f8-5829-417a-8034-2acd3ca9ece2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this notebook, we demonstrates how to evaluate a RAG application using Amazon Bedrock and RAGAS framework. We started by uploaded the sample texts into an S3 bucket for creating the corresponding vector embeddings. After the data is uploaded to S3, we created a knowledge Base for Bedrock application and integrated it with an OpenSearch serverless collection. We fired a data ingestion job using the Bedrock Agent SDK to create the vector embeddings for the data on S3, and persists the vectors into the given Opensearch serverless collection.\n",
    "\n",
    "To perform RAG evaluation on both the standard RAG and the two stage retrieval with a reranking model approach, we used an open source framework RAGAS focusing on faithfullness, answer correctness,, answer similarity, answer_relevancy, context_precision and context_racall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af527e6b-4f78-4234-9c78-4d88490bec28",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "If you are done with the experiment, you can delete the resources used in this notebook by running the following cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a27267-096f-42bb-a4f6-c7a05fb743e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = bedrock_agent.delete_knowledge_base(\n",
    "    knowledgeBaseId=knowledge_base_id\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "673df1fe-eb6c-46ea-9a73-a96e7ae7942e",
   "metadata": {},
   "source": [
    "# RAG Retrieval Optimization - Sentence Window Parsing technique with Amazon Bedrock and Llamaindex\n",
    "\n",
    "Sentence window is another technique that enhances the retrieval process by focusing on individual sentences while providing surrounding context. In this approach, documents are parsed into single sentences, each with a \"window\" of surrounding sentences. During retrieval, the system finds the most relevant individual sentences. However, instead of using only these single sentences, it replaces them with their corresponding windows, which include a specified number of sentences before and after the retrieved sentence. This method allows for more fine-grained retrieval of specific information while still providing necessary context, potentially improving the relevance and coherence of the generated responses.\n",
    "\n",
    "In this lab, we demonstrated how to use sentence window technique for post-retrieval with LlamaIndex. Specifically, we employed the SentenceWindowNodeParser module to splits Amazon's SEC filing documents into individual sentences, creating a node for each sentence while also including a configurable \"window\" of surrounding sentences in the node's metadata. We can then use the MetadataReplacementPostProcessor module to retrieve the sentence along with associated 'window' metadata to improve the context for final response generation.\n",
    "\n",
    "Here are the components we used:Here are the components we used:\n",
    "\n",
    "- Vector Database (Faiss / local)\n",
    "- LLM (Amazon Bedrock - Claude3 Sonnet)\n",
    "- Embeddings Model (Bedrock Titan Text Embeddings v2.0)\n",
    "- Datasets ( Amazons 10-k sec filings from year 2022 and 2023 )\n",
    "- Llamaindex SentenceWindowNodeParser (This example is built on referece llamaindex documentation available at - https://docs.llamaindex.ai/en/stable/examples/node_postprocessor/MetadataReplacementDemo/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464b159c",
   "metadata": {},
   "source": [
    "Small to Large Retrieval (Reference - https://docs.llamaindex.ai/en/stable/optimizing/production_rag/)\n",
    "\n",
    "![alt text](sentence-window.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca61618-2ce2-4001-aac4-ff026d8e544a",
   "metadata": {},
   "source": [
    "## Pre-req\n",
    "You must run the `[workshop_setup.ipynb]`(../lab00-setup/workshop_setup.ipynb) notebook in `lab00-setup` before starting this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c658ca-951f-43d3-9199-73f395957d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.warn(\"Warning: if you did not run lab00-setup, please go back and run the lab00 notebook\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff000c3a",
   "metadata": {},
   "source": [
    "### > Setup\n",
    "We start by importing necessary llamaindex libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebb89a-58e0-4fa0-ba8c-fdceb8f09d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.core import Settings\n",
    "from IPython.display import Markdown, display, HTML\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99781aad",
   "metadata": {},
   "source": [
    "We select Anthropic Claude3 Sonnet as our LLM. For embedding model, we are selecting Amazon Titan Text Embed v2.0. \n",
    "Note that we are using Llamaindex's SentenceWindowNodeParser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47283b-025e-4874-88ed-76245b22f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding, Models\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "import nest_asyncio\n",
    "\n",
    "# create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "# base node parser is a sentence splitter\n",
    "text_splitter = SentenceSplitter()\n",
    "\n",
    "llm = Bedrock(model = \"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
    "embed_model = BedrockEmbedding(model = \"amazon.titan-embed-text-v2:0\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 256\n",
    "Settings.text_splitter = text_splitter\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf99898",
   "metadata": {},
   "source": [
    "### > Document Ingestion\n",
    "We ingest and index the data stored in data directory. The amazon folder has SEC-10k files from 2022 and 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cf76d-0358-4a8a-8803-a09a99d93763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "amazon_secfiles = SimpleDirectoryReader(input_dir=\"../data/lab03/amazon/\").load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f9fff6-4972-4ae8-a306-e1c9c6d83135",
   "metadata": {},
   "source": [
    "### > Build a vector databases\n",
    "\n",
    "We want to demonstrate quality of the generation with and without Sentence Window.\n",
    "To do that, we will first create a normal index to show Naive RAG, then use `node_parser`\n",
    "to add the expand window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae35b40-13c5-499e-b2d4-20a21a514db0",
   "metadata": {},
   "source": [
    "**Without** sentence window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349ddab-941a-47ac-83ca-4fa22e66dd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_nodes = text_splitter.get_nodes_from_documents(amazon_secfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9854d10-d8f8-4a78-a38b-11bba41baca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_index = VectorStoreIndex(base_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2809e7",
   "metadata": {},
   "source": [
    "**With SentenceWindowNodeParse** from above\n",
    "\n",
    "This may take up to 5 minutes to prepare the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f1449-f3e3-47a0-b763-a8baf3ae8e03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes = node_parser.get_nodes_from_documents(amazon_secfiles)\n",
    "\n",
    "sentence_window_index = VectorStoreIndex(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817dfa56-bd36-42ef-803e-bfb7ef391a81",
   "metadata": {},
   "source": [
    "### Test Using Naive RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e946ce-0d86-4df8-9294-05065180ee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Whats Amazons ownership stake in Rivian?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8a05aa-f79d-49fd-b999-90a201ec9916",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = base_index.as_query_engine(similarity_top_k=2)\n",
    "naive_response = query_engine.query(query)\n",
    "\n",
    "print(colored(naive_response, \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a60457-ffbb-43c2-a987-fde93f4a5a9f",
   "metadata": {},
   "source": [
    "### > Test RAG Using Sentence Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22ffd0a-8f1c-43ec-9a18-4f960140027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "query_engine = sentence_window_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")\n",
    "sentence_window_response = query_engine.query(query)\n",
    "\n",
    "print(colored(sentence_window_response, \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3281f5",
   "metadata": {},
   "source": [
    "### > Display the results side-by-side \n",
    "\n",
    "Notice the answer with sentence widow is more accurate and more relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa24bb0e-d36b-4dfe-8f17-9058b06fae4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create the first table\n",
    "df = pd.DataFrame({\n",
    "    'Sentence Window': [query, sentence_window_response],\n",
    "    'Naive RAG': [query, naive_response]\n",
    "})\n",
    "\n",
    "output=\"\"\n",
    "output += df.style.hide()._repr_html_()\n",
    "# output += \"&nbsp;\"\n",
    "\n",
    "display(HTML(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f62852",
   "metadata": {},
   "source": [
    "### > Reivew the Sentence and Window\n",
    "\n",
    "Let's take a look at the senteces and their corresponding window used as the context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924921f-1f14-417e-83ab-1f0d10cd7dd2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for source_node in sentence_window_response.source_nodes:\n",
    "    \n",
    "    print(colored(\"\\nSentence: \\n\", \"green\"))\n",
    "\n",
    "    print(source_node.node.metadata[\"original_text\"])\n",
    "    \n",
    "    print(colored(\"\\nWindow: \\n\", \"green\"))\n",
    "    print(source_node.node.metadata[\"window\"])\n",
    "\n",
    "    print(colored(\"\\n-------------------\\n\", \"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782bc02-0318-4f17-a8bc-4e8f69f4c725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

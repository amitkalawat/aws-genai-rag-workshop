{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ef53ee9-73af-493f-afc8-7b7615ed7bf8",
   "metadata": {},
   "source": [
    "# Prepare Video For RAG\n",
    "Now that you have a good understanding of how we contextualize long-form video assets, in this part of the lab, we will build a Retrieval Augmented Generation (RAG) assistant that can answer questions about the video and help us find relevant clips. This notebook will guide you through the following steps: 1) Contextualize a list of videos using the same technique as the previous notebook. 2) Store scene-level documents with metadata such as video name, start and end timestamps, etc. 3) Upload these documents to S3 as the knowledge source to create a semantic embeddings layer. 4) Create a Bedrock knowledge base using OpenSearch Serverless and ingest the data. 5) Finally, create a RAG assistant to test the videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947a50e2-71a9-457f-bb40-480a818a7ef2",
   "metadata": {},
   "source": [
    "## Pre-req\n",
    "You must run the [workshop_setup.ipynb](../lab00-setup/workshop_setup.ipynb) notebook in `lab00-setup` before starting this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365fabe2-2731-4ff2-86d3-00999bf13606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.warn(\"Warning: if you did not run lab00-setup, please go back and run the lab00 notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b845c7f-d76e-4cda-9cf1-36cf3ee96003",
   "metadata": {},
   "source": [
    "## Load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34413b58-9d9e-4be4-8fbb-ee01e6ac71ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"load bucket, region, and role....\\n\")\n",
    "# bucket and parameter stored from Initial setup lab00\n",
    "%store -r bucket\n",
    "%store -r role\n",
    "%store -r region\n",
    "%store -r video_prep_prefix\n",
    "\n",
    "## check all 5 values are printed and do not fail\n",
    "print(bucket)\n",
    "print(role)\n",
    "print(region)\n",
    "print(video_prep_prefix)\n",
    "\n",
    "print(\"\\nload the vector db parameters....\\n\")\n",
    "# vector parameters stored from Initial setup lab02\n",
    "%store -r vector_store_name\n",
    "%store -r vector_collection_arn\n",
    "%store -r vector_collection_id\n",
    "%store -r vector_host\n",
    "%store -r bedrock_kb_execution_role_arn\n",
    "## check all 4 values are printed and do not fail\n",
    "print(vector_store_name)\n",
    "print(vector_collection_arn)\n",
    "print(vector_collection_id)\n",
    "print(vector_host)\n",
    "print(bedrock_kb_execution_role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b737a-0be0-446b-97e7-ed97d3533203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from lib import s3_helper as s3h\n",
    "from termcolor import colored\n",
    "from lib import util\n",
    "from lib import video_helper as vh\n",
    "from lib import ffmpeg_helper as ffh\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Video\n",
    "from sagemaker.utils import name_from_base\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "scene_doc_dir = \"scene_documents\"\n",
    "prefix = video_prep_prefix\n",
    "util.mkdir(scene_doc_dir)\n",
    "\n",
    "boto3_session = boto3.Session()\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region)\n",
    "\n",
    "\n",
    "# opensearch service\n",
    "credentials = boto3_session.get_credentials()\n",
    "service = 'aoss'\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region, service)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250893a4-bfaf-4ab8-ac34-45e4e943566e",
   "metadata": {},
   "source": [
    "### > Video Files To Process\n",
    "\n",
    "[Meridian](https://opencontent.netflix.com/) is under the [Creative Commons Attribution 4.0 International Public License](https://creativecommons.org/licenses/by/4.0/legalcode)\n",
    "[ASC StEM2 “The Mission”](https://dpel.aswf.io/asc-stem2/) is under the [ASWF Digital Assets License v1.1](https://raw.githubusercontent.com/AcademySoftwareFoundation/foundation/main/digital_assets/aswf_digital_assets_license_v1.1.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c49062-2b09-4a68-b9a8-96e40e5f47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [\n",
    "    \"https://dx2y1cac29mt3.cloudfront.net/mp4/netflix/Netflix_Open_Content_Meridian.mp4\",\n",
    "    \"https://dpel-assets.aswf.io/asc-stem2/ASC_StEM2_178_2K_24_100nits_Rec709_Stereo.mp4\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e240ff-9e61-44f3-92b0-7b90ced2b171",
   "metadata": {},
   "source": [
    "We created helper functions to go through the end-to-end contextualization process quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c10bfd8-8a8d-4a19-906f-5cb786140923",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in videos:\n",
    "    file_name = Path(video).name\n",
    "    !curl {video} -o {file_name}\n",
    "    video_dir = Path(video).stem\n",
    "\n",
    "    display(Video(file_name, width=640, height=360))\n",
    "    # upload video to S3\n",
    "    response = s3h.upload_object(bucket, \"contextual_ad\", file_name)\n",
    "\n",
    "    stream_info = ffh.probe_stream(file_name)\n",
    "\n",
    "    # generate chapter segements\n",
    "    conversations, transcribe_cost, conversation_cost = vh.generate_chapeter_segements(file_name, \n",
    "                                                                                        video_dir,\n",
    "                                                                                        bucket,\n",
    "                                                                                        stream_info['video_stream']['duration_ms'])\n",
    "\n",
    "    # generate scene segements\n",
    "    shots_in_scenes, frames_in_shots, frame_embeddings, frame_embeddings_cost = vh.group_scene_segements(file_name, \n",
    "                                                                                                         video_dir, \n",
    "                                                                                                         stream_info)\n",
    "\n",
    "    # align chapter and scenes\n",
    "    scenes_in_chapters, shots_in_scenes, frames_in_shots, frame_embeddings = vh.align_chapters_n_scenes(video_dir,\n",
    "                                                                                                        conversations, \n",
    "                                                                                                        shots_in_scenes, \n",
    "                                                                                                        frames_in_shots, \n",
    "                                                                                                        frame_embeddings)\n",
    "\n",
    "    # contextualize scene and generate scene docs\n",
    "    frames_in_chapters = vh.get_chapter_frames(frame_embeddings, scenes_in_chapters)\n",
    "    \n",
    "    contextual_cost = vh.generate_contextual_output(file_name, \n",
    "                                                   video_dir, \n",
    "                                                   scene_doc_dir, \n",
    "                                                   scenes_in_chapters,\n",
    "                                                   frames_in_chapters)\n",
    "\n",
    "    total_estimated_cost = 0\n",
    "\n",
    "    for estimated_cost in [transcribe_cost, conversation_cost, frame_embeddings_cost, contextual_cost]:\n",
    "        total_estimated_cost += estimated_cost['estimated_cost']\n",
    "    total_estimated_cost = round(total_estimated_cost, 4)\n",
    "    \n",
    "    print('\\n========================================================================\\n')\n",
    "    print('Total estimated cost:', colored(f\"${total_estimated_cost}\", 'green'))\n",
    "    print('\\n========================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c31c48-fe02-425a-9dc7-e8a961318557",
   "metadata": {},
   "source": [
    "### > Upload documents to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92026d83-ca70-4cee-a892-008ed321bbdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 sync {scene_doc_dir} s3://{bucket}/{prefix}/{scene_doc_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22944f-9cff-4b15-b03c-36dced3b65bb",
   "metadata": {},
   "source": [
    "## Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "For this lab, we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884f74ae-1729-44fc-a6f3-39b2872b9d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_client = boto3_session.client('opensearchserverless')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f3f893-dcc1-465c-b534-3da9fb77f896",
   "metadata": {},
   "source": [
    "### Create the schema for vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f36e48-3bfc-4e17-9603-b3cac119cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = name_from_base(\"video-prep\")\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\"\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1024,\n",
    "            \"method\": {\n",
    "                \"name\": \"hnsw\",\n",
    "                \"space_type\": \"innerproduct\",\n",
    "                \"engine\": \"faiss\",\n",
    "                \"parameters\": {\n",
    "                  \"ef_construction\": 256,\n",
    "                  \"m\": 48\n",
    "                }\n",
    "             }\n",
    "         },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         \n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': vector_host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994f5a7-2f85-46fb-a07c-039eadcaba6f",
   "metadata": {},
   "source": [
    "### Create the index in OSS collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c812be5f-ed05-4736-bf29-06c9200fb75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create index\n",
    "response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "print('\\nCreating index:')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261aa81a-b7cd-43c7-8e0c-d5b5a4b157af",
   "metadata": {},
   "source": [
    "## Create Knowledge Base\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the s3 configuration, which will be used to create the data source object later.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c7e7a-92d9-47bb-ae65-6a2b9840a730",
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": vector_collection_arn,\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "chunkingStrategyConfiguration = {\n",
    "    \"chunkingStrategy\": \"NONE\",\n",
    "}\n",
    "\n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket}\",\n",
    "    \"inclusionPrefixes\":[f\"{prefix}/{scene_doc_dir}/\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "}\n",
    "\n",
    "embeddingModelArn = f\"arn:aws:bedrock:{region}::foundation-model/amazon.titan-embed-text-v2:0\"\n",
    "\n",
    "kb_name = name_from_base(\"video-knowledge-base\")\n",
    "description = \"Video Search knowledge base.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c755f2d7-41e1-4dc6-a2dd-b78812efe819",
   "metadata": {},
   "source": [
    "Provide the above configurations as input to the `create_knowledge_base` method, which will create the Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ba45d2-3272-48a8-b416-1300d689436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "        name = kb_name,\n",
    "        description = description,\n",
    "        roleArn = bedrock_kb_execution_role_arn,\n",
    "        knowledgeBaseConfiguration = {\n",
    "            \"type\": \"VECTOR\",\n",
    "            \"vectorKnowledgeBaseConfiguration\": {\n",
    "                \"embeddingModelArn\": embeddingModelArn\n",
    "            }\n",
    "        },\n",
    "        storageConfiguration = {\n",
    "            \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "            \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "        }\n",
    "    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df863cf1-d597-411e-93d1-383e2262e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbfd7b-56c3-4ee3-b69d-5fc64a162eb2",
   "metadata": {},
   "source": [
    "Next we need to create a data source, which will be associated with the knowledge base created above. Once the data source is ready, we can then start to ingest the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02096573-4697-4895-83d4-826b20974a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79a7ff-eda6-474d-9825-575199e8b21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "    name = kb_name,\n",
    "    description = description,\n",
    "    knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "    dataSourceConfiguration = {\n",
    "        \"type\": \"S3\",\n",
    "        \"s3Configuration\":s3Configuration\n",
    "    },\n",
    "    vectorIngestionConfiguration = {\n",
    "        \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "    }\n",
    ")\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd546c2-0550-4c3a-9472-7d9a761a914e",
   "metadata": {},
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source is created, we can start the ingestion job.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6685df67-f4e9-4889-837e-d43fb752084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an ingestion job\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6600f2-d2c8-4401-b407-3e345f73091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17deee58-d96b-42fe-9e8b-cf592bb16f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "  get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "  job = get_job_response[\"ingestionJob\"]\n",
    "print(job)\n",
    "time.sleep(80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c99ea-59b1-4765-b689-ec322c8d3ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "%store kb_id\n",
    "print(kb_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daf904c-7123-41da-b8a9-a0a497d4f4e8",
   "metadata": {},
   "source": [
    "## Test the knowledge base\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1415693e-88d7-476c-815c-d4538cd0c9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out KB using RetrieveAndGenerate API\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region)\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\" # try with both claude instant as well as claude-v2. for claude v2 - \"anthropic.claude-v2\"\n",
    "model_arn = f'arn:aws:bedrock:{region}::foundation-model/{model_id}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10b5239-3b27-4c26-aaa4-9352f73766a0",
   "metadata": {},
   "source": [
    "Utility function to help display the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0954f-7f85-4943-a182-36870ae0ce04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_video(video_path, start_time=0, width=640, height=360):\n",
    "    control_id = name_from_base(\"id\")\n",
    "\n",
    "    display(HTML(f\"\"\"\n",
    "    <video alt=\"test\" controls id=\"{control_id}\" width=\"{width}\" height=\"{height}\" >\n",
    "      <source src=\"{video_path}\">\n",
    "    </video>\n",
    "    \n",
    "    <script>\n",
    "    video = document.getElementById(\"{control_id}\")\n",
    "    video.currentTime = {start_time};\n",
    "    </script>\n",
    "    \"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3805d20c-0447-4f99-b34f-bf9a88ce44c6",
   "metadata": {},
   "source": [
    "### > Sample Questions\n",
    "- from ASC_StEM2_178_2K_24_100nits_Rec709_Stereo, where is the scene where group of people running through a rugged and arid desert\n",
    "- from ASC_StEM2_178_2K_24_100nits_Rec709_Stereo, find me a car is running away from a hotstil situation.\n",
    "- from the Meridian video, show me scene where 'Virgin Soil Pictures Production' logo appears\n",
    "- from the Meridian video, show me scene when the live credits scene appears\n",
    "- from the Meridian video, show me an office setting where people having serious discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba515e2e-f5b0-42aa-b3d5-7fbf4d372fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "query = \"from the Meridian video, show me an office setting where people having serious discussion\"\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        'text': query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "generated_text = response['output']['text']\n",
    "\n",
    "display(Markdown(generated_text))\n",
    "\n",
    "if \"citations\" in response:\n",
    "    references = response[\"citations\"][0][\"retrievedReferences\"]\n",
    "    if len(references)>0:\n",
    "        for ref in references:\n",
    "            video = ref['metadata']['video']\n",
    "            start = ref['metadata']['start']\n",
    "            print(video, start)\n",
    "            display_video(video, start_time=int(start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e50acf-b287-45e3-a465-799d1b9a2510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

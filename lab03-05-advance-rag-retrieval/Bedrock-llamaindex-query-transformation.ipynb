{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cea58c-48bc-4af6-8358-df9695659983",
   "metadata": {},
   "source": [
    "# RAG Retrieval Optimization - Query Rewriting using Amazon Bedrock and Llamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673df1fe-eb6c-46ea-9a73-a96e7ae7942e",
   "metadata": {},
   "source": [
    "Query rewrite in RAG is a technique that reformulates or transforms the original user query to improve the retrieval process and ultimately enhance the quality of generated responses. This strategy involves modifying the input query in various ways, such as expanding it with related terms, simplifying complex queries, or breaking them down into sub-questions. The goal is to bridge the gap between the user's natural language input and the system's ability to find relevant information in the knowledge base. By rewriting queries, RAG systems can increase both recall (retrieving more relevant documents) and precision (improving the relevance of retrieved information), leading to more accurate and comprehensive answers.\n",
    "\n",
    "In this lab, we will build a query engine using LlamaIndex to answering a complex query about Amazon's 10K SEC filing from years 2022 and 2023. The lab uses the SubQuestionQueryEngine module from LLamaIndex to first breaks down the complex query into sub questions for each relevant data source then gathers all the intermediate responses and synthesizes to get better final response from Amazon's 10K documents.\n",
    "\n",
    "Here are the components we used:\n",
    "\n",
    "- Vector Database (Faiss / local)\n",
    "- LLM (Amazon Bedrock - Nova Pro)\n",
    "- Embeddings Model (Bedrock Titan Text Embeddings v2.0)\n",
    "- Datasets ( Amazons 10-k sec filings from year 2022 and 2023 )\n",
    "- Llamaindex  (This example is built on referece llamaindex documentation available at - https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3556ea7-f28f-440e-ab45-f18ae3fe0889",
   "metadata": {},
   "source": [
    "## Pre-req\n",
    "You must run the `[workshop_setup.ipynb]`(../lab00-setup/workshop_setup.ipynb) notebook in `lab00-setup` before starting this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77b5d08-2928-4cb7-abfd-5f7d089053dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.warn(\"Warning: if you did not run lab00-setup, please go back and run the lab00 notebook\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4523f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the config content\n",
    "config_content = \"\"\"[profile default]\n",
    "region = us-west-2\n",
    "output = json\n",
    "\"\"\"\n",
    "\n",
    "# Get the path to the .aws directory and config file\n",
    "home_dir = \"/home/sagemaker-user\"  # SageMaker specific path\n",
    "aws_dir = os.path.join(home_dir, \".aws\")\n",
    "config_path = os.path.join(aws_dir, \"config\")\n",
    "\n",
    "# Check if config file already exists\n",
    "if os.path.exists(config_path):\n",
    "    print(f\"AWS config file already exists at {config_path}. No changes made.\")\n",
    "else:\n",
    "    # Create the .aws directory if it doesn't exist\n",
    "    os.makedirs(aws_dir, exist_ok=True)\n",
    "    \n",
    "    # Create the config file with the content\n",
    "    with open(config_path, \"w\") as f:\n",
    "        f.write(config_content + \"\\n\")\n",
    "    \n",
    "    print(f\"Created directory {aws_dir} and created AWS config file at {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef30d8c",
   "metadata": {},
   "source": [
    "### > Setup\n",
    "We start by importing necessary llamaindex libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebb89a-58e0-4fa0-ba8c-fdceb8f09d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.core import Settings\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6bbb0",
   "metadata": {},
   "source": [
    "We select Amazon Nova Pro as our LLM. For embedding model, we are selecting Amazon Titan Text Embed v2.0. Chunk size is set at 512 for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47283b-025e-4874-88ed-76245b22f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.bedrock_converse import BedrockConverse\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding, Models\n",
    "\n",
    "profile_name = \"default\"\n",
    "\n",
    "# define the LLM\n",
    "llm = BedrockConverse(\n",
    "    model=\"us.amazon.nova-pro-v1:0\",\n",
    "    profile_name=profile_name,\n",
    ")\n",
    "\n",
    "# define the embedding model\n",
    "embed_model = BedrockEmbedding(model = \"amazon.titan-embed-text-v2:0\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63f351",
   "metadata": {},
   "source": [
    "### > Document Ingestion\n",
    "We ingest the documents and use [Titan Text Embeddings v2.0 model](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html) to create the embedding for each document chunk. The amazon folder has SEC-10k files from 2022 and 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cf76d-0358-4a8a-8803-a09a99d93763",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "amazon_secfiles = SimpleDirectoryReader(input_dir=\"../data/lab03/amazon/\").load_data()\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    amazon_secfiles,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534031e3-83be-4e85-8894-88cc8e24384b",
   "metadata": {},
   "source": [
    "Define the query engine from index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a91344c-8593-46c4-83a3-03cd39226295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build index and query engine\n",
    "vector_query_engine = vector_index.as_query_engine(\n",
    "    top_k=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65426307-171f-48b8-8b30-442ac03af29c",
   "metadata": {},
   "source": [
    "### > Test Using Naive RAG\n",
    "Let's try a common multi-part question on the 10K document using the naive RAG approach...\n",
    "\n",
    "Example questions\n",
    "- What are amazons key priorities before, during and after covid?\n",
    "- What were key challenges faced by Amazon in year 2022 and 2023?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556187d1-bedd-4cda-94c9-083fe5adc6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are amazons key priorities before, during and after covid?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02b9ce-392e-44f5-b448-8298db71a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_response = vector_query_engine.query(query)\n",
    "print(colored(raw_response, \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e3802",
   "metadata": {},
   "source": [
    "### > Test Using SubQuestionQueryEngine to rewrite the query\n",
    "\n",
    "`SubQuestionQueryEngine` is a llamaindex module designed to tackle the problem of answering a complex query using multiple data sources.\n",
    "It first breaks down the complex query into sub questions for each relevant data source, then gather all the intermediate reponses and synthesizes a final response.\n",
    "\n",
    "In this example, the `SubQuestionQueryEngine` will use the LLM model (Claude3 Sonnet) to breaking down of complex queries into sub-queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414cda3-8a56-44a8-96a8-522c0f0e10b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup base query engine as tool\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=vector_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"Amazon-10k\",\n",
    "            description=\"Amazon SEC 10-k filings for years 2022 and 2023\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e68ef-3065-416f-8555-711897998c7f",
   "metadata": {},
   "source": [
    "Run the same query using `SubQuestionQueryEngine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826956e5-2206-4ba0-8aeb-beafa9a284b2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformed_response = query_engine.query(query)\n",
    "print(\"\\n\")\n",
    "print(colored(transformed_response, \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d8c864-1b7e-45bf-85bb-bfaa78cad398",
   "metadata": {},
   "source": [
    "### > Display the results side-by-side \n",
    "\n",
    "Notice splitting the question into sub questions increase your chances of matching the right and complete information and generate a more comprehensive final answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d70e64-769e-485d-9710-da8fa556bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Create the first table\n",
    "df = pd.DataFrame({\n",
    "    'Naive RAG': [query, raw_response],\n",
    "    'RAG w/ Query Rewrite': [query, transformed_response]\n",
    "})\n",
    "\n",
    "output=\"\"\n",
    "output += df.style.hide().set_table_attributes(\"style='display:inline'\")._repr_html_()\n",
    "output += \"&nbsp;\"\n",
    "\n",
    "display(HTML(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

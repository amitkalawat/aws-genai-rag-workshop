{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c9b8f0a-463c-4281-906d-09bbf81bc52e",
   "metadata": {},
   "source": [
    "## Data Preparation For Image Search\n",
    "\n",
    "Multi-modal data, such as images and diagrams, represent a significant untapped resource in most current Retrieval-Augmented Generation (RAG) systems. While these systems excel at processing and generating text, they often overlook the wealth of information contained in visual formats. Images and diagrams can convey complex concepts, relationships, and trends in ways that text alone cannot match. By incorporating multi-modal data, RAG systems could unlock a treasure trove of insights, enabling more comprehensive understanding and analysis.\n",
    "\n",
    "In the lab, you will explore 2 different techniques for handling image data for RAG: multi-modal embedding and grounding images with text. \n",
    "\n",
    "- **Multi-modal embedding** directly encodes images into vector representations. This approach preserves the visual information and can capture nuanced features that might be lost in text descriptions. However, it requires specialized models and may struggle with complex or abstract visual concepts.\n",
    "\n",
    "- **Grounding images** with text involves generating textual descriptions or captions for images, then using these descriptions for text-based embedding. This method leverages the power of existing text embedding models and can provide more interpretable representations. It's particularly effective for images with clear, describable content. However, it may lose some fine-grained visual details and is dependent on the quality of the image-to-text conversion.\n",
    "\n",
    "- Note: You can also use both and adopt a combined strategy. This method can provide a more comprehensive representation, capturing both visual and textual aspects of the data. It allows for flexible querying and can potentially improve retrieval accuracy. The downside is increased computational complexity and storage requirements\n",
    "\n",
    "You will first analyze the top K precision for two techniques on provided image data: 1) simple images synthetically generated by the Amazon Titan Image Generator Model, and 2) complex images (architecture diagrams) from the [AWS Solutions Library](https://aws.amazon.com/solutions/). After comparing the pros and cons of both approaches, we will build a Naive RAG using Amazon Bedrock to retrieve these images using natural language queries.\n",
    "\n",
    "| |  |\n",
    "|----------|----------|\n",
    "| ![Image 1](static/multi-modal.png)| ![Image 2](static/ground-to-text.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bc3140-bfca-47b2-bc87-afae112b4f37",
   "metadata": {},
   "source": [
    "## Pre-req\n",
    "You must run the [workshop_setup.ipynb](../lab00-setup/workshop_setup.ipynb) notebook in `lab00-setup` before starting this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da1e877-ffae-4b52-894a-454cbec17964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.warn(\"Warning: if you did not run lab00-setup, please go back and run the lab00 notebook\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898ec47-9b58-4022-acbd-0ca60c3e7cbc",
   "metadata": {},
   "source": [
    "## Load the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9bb2bd-cffc-410d-95a3-08806064a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"load the data parameters....\\n\")\n",
    "# bucket and parameter stored from Initial setup lab01\n",
    "%store -r root_dir\n",
    "%store -r jsonl_files\n",
    "\n",
    "## check all 5 values are printed and do not fail\n",
    "print(root_dir)\n",
    "print(jsonl_files)\n",
    "\n",
    "print(\"\\nload the vector db parameters....\\n\")\n",
    "\n",
    "# vector parameters stored from Initial setup\n",
    "%store -r vector_host\n",
    "%store -r vector_collection_arn\n",
    "%store -r vector_collection_id\n",
    "%store -r bedrock_kb_execution_role_arn\n",
    "\n",
    "## check all 4 values are printed and do not fail\n",
    "print(vector_host)\n",
    "print(vector_collection_arn)\n",
    "print(vector_collection_id)\n",
    "print(bedrock_kb_execution_role_arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7658c-9224-4836-a0d3-a84cc1e1d4de",
   "metadata": {},
   "source": [
    "### > Initialize parameters and import helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344c7f63-93e5-4f5f-8a10-c5a7e60f965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "from PIL import Image\n",
    "import time\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from sagemaker.utils import name_from_base\n",
    "from opensearch_util import OpenSearchManager\n",
    "\n",
    "from helper import (\n",
    "    _encode,\n",
    "    download_file_from_s3,\n",
    "    get_mm_embedding,\n",
    "    get_text_embedding,\n",
    "    evaluate_top_hit\n",
    ")\n",
    "\n",
    "os_manager = OpenSearchManager()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae2c2db-ed97-41d2-8639-19d0c150617e",
   "metadata": {},
   "source": [
    "## > Load image manifest file\n",
    "\n",
    "You will first load both simple and complex image manifest file. These manifest file contain images information under `corpus`, question to search the image under `queries`, and groundtruth mapping between `corpus` and `queries` under `relevant_docs`. \n",
    "\n",
    "Here is the structure of the manifest file. image info under `corpus` contains the local and s3 location of the image, and the image caption. \n",
    "```\n",
    "{\n",
    "    corpus:{\n",
    "        <image_id>:{\n",
    "            \"image-ref\": <S3_location>,\n",
    "            \"image-path\": <local_path>,\n",
    "            \"caption\": <image_caption>,\n",
    "        }\n",
    "        ...\n",
    "    }\n",
    "    queries:{\n",
    "        <query_id>:<sample_query>,\n",
    "        ....\n",
    "    },\n",
    "    relevant_docs:{\n",
    "        <query_id>:[<image_id>, ...],\n",
    "        ...\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "After the manifest files are loaded, you will create four lists of index objects with embeddings: \n",
    "\n",
    "1. simple image - multimodal embedding\n",
    "2. complex image - multimodal embedding\n",
    "3. simple image - text embedding of the image caption\n",
    "4. complex image - text embedding of the image caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c76c74b-0892-4f75-a803-87b2ed19943d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "indexes=[]\n",
    "for jsonl in jsonl_files:\n",
    "\n",
    "    print(f\"Prepare image manifest file: {jsonl}\")\n",
    "    \n",
    "    jsonl_path = os.path.join(root_dir, jsonl)\n",
    "    \n",
    "    with open(jsonl_path, 'r+') as f:\n",
    "        dataset = json.load(f)\n",
    "        \n",
    "    image_data = dataset['corpus']\n",
    "\n",
    "    for model_id in [\"amazon.titan-embed-image-v1\", \"amazon.titan-embed-text-v2:0\"]:\n",
    "        \n",
    "        index_obj = dict()\n",
    "        index_obj[\"file\"] = jsonl.split(\"/\")[-1]\n",
    "        index_obj[\"model_id\"] = model_id\n",
    "        index_obj[\"image_data\"] = []\n",
    "        index_obj[\"dataset\"] = dataset\n",
    "        \n",
    "        for i, key in enumerate(image_data):           \n",
    "            \n",
    "            metadata = dict()\n",
    "        \n",
    "            metadata['id'] = key\n",
    "        \n",
    "            image = download_file_from_s3(image_data[key]['image-ref'])\n",
    "            image_base64 = _encode(image)\n",
    "        \n",
    "            metadata['image-ref'] = image_data[key]['image-ref']\n",
    "        \n",
    "            metadata['caption'] = image_data[key]['caption']\n",
    "    \n",
    "            if model_id == \"amazon.titan-embed-image-v1\":\n",
    "                metadata['vector_field'] = get_mm_embedding(image_base64=image_base64)\n",
    "                index_obj[\"image_data\"].append(metadata)\n",
    "                \n",
    "            else:\n",
    "                metadata['vector_field'] = get_text_embedding(image_data[key]['caption'], model_id=\"amazon.titan-embed-text-v2:0\")\n",
    "                index_obj[\"image_data\"].append(metadata)\n",
    "\n",
    "        indexes.append(index_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6180fc-9f91-46d3-9f16-d3b2089b0082",
   "metadata": {},
   "source": [
    "### > Create a vector index\n",
    "\n",
    "You then iterating over the 4 different type of index lists, creating the vector index in Opensearch and then bulk ingest the image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9afdb-7232-46f3-ae47-9ee1fef964c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os_manager.initialize_client(host=vector_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6be38-e7e6-426b-ad86-f2926f70b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_body = {\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"id\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"image-ref\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"caption\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"vector_field\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 1024,\n",
    "        \"method\": {\n",
    "          \"engine\": \"nmslib\",\n",
    "          \"space_type\": \"cosinesimil\", \n",
    "          \"name\": \"hnsw\",\n",
    "          \"parameters\": {\n",
    "            \"ef_construction\": 512,\n",
    "            \"m\": 16\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"settings\": {\n",
    "    \"index\": {\n",
    "      \"number_of_shards\": 2,\n",
    "      \"knn.algo_param\": {\n",
    "        \"ef_search\": 512\n",
    "      },\n",
    "      \"knn\": True\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea7aa45-504d-4d90-a8a2-bb116ade4a2c",
   "metadata": {},
   "source": [
    "### > Bulk ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a5781-9509-4295-acda-28103848982c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in indexes:\n",
    "    index_name = name_from_base(f\"{index['file'].split('_')[0]}-{index['model_id'].split('.')[-1]}\".replace(':0', ''))\n",
    "\n",
    "    index[\"index_name\"] = index_name\n",
    "\n",
    "    resp = os_manager.create_index(index_name=index_name, index_body=index_body)\n",
    "    time.sleep(40)\n",
    "    \n",
    "    sucess, failed = os_manager.bulk_index_ingestion(index_name=index_name,\n",
    "                                                     data=index[\"image_data\"])\n",
    "    \n",
    "    print(f\"number of record successfully ingested: {sucess}, failed: {failed}\")\n",
    "    time.sleep(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e4bba-f8a4-4c2f-86da-ae04a298a079",
   "metadata": {},
   "source": [
    "### > Opensearch query template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a552242-ad49-474d-b693-06d4a7cdd38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build opensearch query\n",
    "os_query = {\n",
    "    \"size\": 5,\n",
    "    \"query\":{\n",
    "        \"knn\": {\n",
    "        \"vector_field\": {\n",
    "            \"vector\": [],\n",
    "            \"k\": 5\n",
    "        }\n",
    "        }\n",
    "    },\n",
    "    \"_source\": [\"id\", \n",
    "                \"image-ref\", \n",
    "                \"caption\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a46df17-8272-4b08-8f85-2d1d85a13df5",
   "metadata": {},
   "source": [
    "### > Run Top K Precision Benchmark\n",
    "\n",
    "Once the indexes are ready, you can perform an evaluation of image retrieval performance across multiple indexes and models. It iterates through a list of indexes, and for each index, it calls an evaluate_top_hit function to calculate the percentage of top hits (correct matches) for different values of k (1, 5, and 10). \n",
    "\n",
    "The resuls is compared with the groundtruth in `relevant_docs` data to get a percentage of top hits for each value of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b893ef7a-5630-4f30-a41f-7879f50c09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = []\n",
    "for index in indexes:\n",
    "    \n",
    "    test_output = dict()\n",
    "    model_id = index[\"model_id\"]\n",
    "    index_name =index[\"index_name\"]\n",
    "    test_output[\"index_name\"] = index_name\n",
    "    \n",
    "    for k in [1, 5, 10]:\n",
    "\n",
    "        eval_results = evaluate_top_hit(os_manager, os_query, index[\"dataset\"], index_name, top_k=5, model_id=model_id)\n",
    "        df_base = pd.DataFrame(eval_results)\n",
    "        top_hits = df_base['is_hit'].mean()\n",
    "\n",
    "        \n",
    "        test_output[f\"top_{k}\"] = top_hits\n",
    "    \n",
    "        print(f\"{index_name} at top {k}, the percent of top hits: {top_hits*100:.2f} %\")\n",
    "    benchmark.append(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a568a40-de2f-4442-a8a4-0d469f1f27fc",
   "metadata": {},
   "source": [
    "Here are the results: notice that for simple images, both multi-modal and embedding of image captions performed perfectly for top 1, 5, and 10 precision. However, the multi-modal accuracy drops more on complex diagrams as the embedding space is no longer able to pick up all the details in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dad128-9f22-4abf-9b60-8e9d3f89f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.0%}'.format\n",
    "df = pd.DataFrame(benchmark)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0b80f7-5342-4ed8-af55-637d6e9a4e3a",
   "metadata": {},
   "source": [
    "### > Utility function to properly display the retrieved images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88250bc-a801-4cce-a5e5-1a52df8f6d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, HTML, display\n",
    "\n",
    "def display_s3_images_grid(results, images_per_row=5, img_width=600):\n",
    "    \"\"\"\n",
    "    Display images from a list of S3 URLs in a side-by-side grid in a Jupyter notebook output cell.\n",
    "    \n",
    "    Args:\n",
    "    s3_urls (list): A list of S3 URLs in the format 's3://bucket-name/key'\n",
    "    images_per_row (int): Number of images to display per row\n",
    "    img_width (int): Width of each image in pixels\n",
    "    \"\"\"\n",
    "    \n",
    "    html_table = \"<table><tr>\"\n",
    "    for i, image_output in enumerate(results):\n",
    "        \n",
    "        image = download_file_from_s3(image_output[\"_source\"][\"image-ref\"])\n",
    "            \n",
    "        image_base64 = _encode(image)\n",
    "        \n",
    "        # Add image to HTML\n",
    "        html_table += f\"<td style='padding:5px;'><img src='data:image/jpeg;base64,{image_base64}' width='{img_width}px'/></td>\"\n",
    "        \n",
    "        # Start a new row after every 'images_per_row' images\n",
    "        if i+1 % images_per_row == 0:\n",
    "            html_table += \"</tr><tr>\"\n",
    "    \n",
    "    html_table += \"</tr></table>\"\n",
    "    display(HTML(html_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914ec914-c363-4f41-b802-47ba1e892972",
   "metadata": {},
   "source": [
    "### > Create a Naive RAG system for Simple Image data\n",
    "\n",
    "Here are some sample questions:\n",
    "- I want a picture of a kid drawing pictures on the wall\n",
    "- Give me an image of a circus\n",
    "- Picture of animals happily playing instruments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29f744a-fa98-4626-ab8a-97fdca67b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"I want a picture of a kid drawing pictures on the wall\"\n",
    "top_k = 3\n",
    "\n",
    "os_query[\"query\"][\"knn\"][\"vector_field\"][\"vector\"] = get_text_embedding(query, model_id=\"amazon.titan-embed-text-v2:0\")\n",
    "os_query[\"size\"] = top_k\n",
    "os_query[\"query\"][\"knn\"][\"vector_field\"][\"k\"] = top_k\n",
    "\n",
    "for index in indexes:\n",
    "    if \"simple-titan-embed-text\" in index[\"index_name\"]:\n",
    "        results = os_manager.opensearch_query(os_query,\n",
    "                                              index_name=index[\"index_name\"])\n",
    "display_s3_images_grid(results, images_per_row=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4ffb9c-96bd-4fec-ac29-90baf3a488aa",
   "metadata": {},
   "source": [
    "### > Create a Naive RAG system for complex image (architecture diagrams)\n",
    "\n",
    "Here are some sample questions:\n",
    "- a solution that can that can generate slow-motion from existing video\n",
    "- a solution to increase resolution for existing videos\n",
    "- a solution to create personalized avatar images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e44d3-1188-41eb-94ed-9a0320717d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a solution that can that can generate slow-motion from existing video\"\n",
    "top_k = 3\n",
    "\n",
    "os_query[\"query\"][\"knn\"][\"vector_field\"][\"vector\"] = get_text_embedding(query, model_id=\"amazon.titan-embed-text-v2:0\")\n",
    "os_query[\"size\"] = top_k\n",
    "os_query[\"query\"][\"knn\"][\"vector_field\"][\"k\"] = top_k\n",
    "\n",
    "for index in indexes:\n",
    "    if \"complex-titan-embed-text\" in index[\"index_name\"]:\n",
    "        results = os_manager.opensearch_query(os_query,\n",
    "                                          index_name=index[\"index_name\"])\n",
    "        \n",
    "display_s3_images_grid(results, images_per_row=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc34dd0-4d16-40e5-a2ce-6d311792e4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

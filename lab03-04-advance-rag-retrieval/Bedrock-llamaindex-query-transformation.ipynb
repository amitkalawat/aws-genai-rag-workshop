{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cea58c-48bc-4af6-8358-df9695659983",
   "metadata": {},
   "source": [
    "# RAG Retrieval Optimization - Query Rewriting using Amazon Bedrock and Llamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673df1fe-eb6c-46ea-9a73-a96e7ae7942e",
   "metadata": {},
   "source": [
    "Query rewrite in RAG is a technique that reformulates or transforms the original user query to improve the retrieval process and ultimately enhance the quality of generated responses. This strategy involves modifying the input query in various ways, such as expanding it with related terms, simplifying complex queries, or breaking them down into sub-questions. The goal is to bridge the gap between the user's natural language input and the system's ability to find relevant information in the knowledge base. By rewriting queries, RAG systems can increase both recall (retrieving more relevant documents) and precision (improving the relevance of retrieved information), leading to more accurate and comprehensive answers.\n",
    "\n",
    "In this lab, we will build a query engine using LlamaIndex to answering a complex query about Amazon's 10K SEC filing from years 2022 and 2023. The lab uses the SubQuestionQueryEngine module from LLamaIndex to first breaks down the complex query into sub questions for each relevant data source then gathers all the intermediate responses and synthesizes to get better final response from Amazon's 10K documents.\n",
    "\n",
    "- Vector Database (Faiss / local)\n",
    "- LLM (Amazon Bedrock - Claude3 Sonnet)\n",
    "- Embeddings Model (Bedrock Titan Text Embeddings v2.0)\n",
    "- Datasets ( Amazons 10-k sec filings from year 2022 and 2023 )\n",
    "- Llamaindex  (This example is built on referece llamaindex documentation available at - https://docs.llamaindex.ai/en/stable/examples/query_engine/sub_question_query_engine/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef30d8c",
   "metadata": {},
   "source": [
    "### > Setup\n",
    "We start by importing necessary llamaindex libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebb89a-58e0-4fa0-ba8c-fdceb8f09d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af6bbb0",
   "metadata": {},
   "source": [
    "We select Anthropic Claude3 Sonnet as our LLM. For embedding model, we are selecting Amazon Titan Text Embed v2.0. Chunk size is set at 256 for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47283b-025e-4874-88ed-76245b22f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding, Models\n",
    "\n",
    "llm = Bedrock(model = \"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
    "embed_model = BedrockEmbedding(model = \"amazon.titan-embed-text-v2:0\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a28a13d",
   "metadata": {},
   "source": [
    "Using the LlamaDebugHandler to print the trace of the sub questions captured by the SUB_QUESTION callback event type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93623d-4818-4cf2-be82-1444ca86dd3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "Settings.callback_manager = callback_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b63f351",
   "metadata": {},
   "source": [
    "### > Document Ingestion\n",
    "We ingest the documents and use [Titan Text Embeddings v2.0 model](https://docs.aws.amazon.com/bedrock/latest/userguide/titan-embedding-models.html) to create the embedding for each document chunk. The amazon folder has SEC-10k files from 2022 and 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cf76d-0358-4a8a-8803-a09a99d93763",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "amazon_secfiles = SimpleDirectoryReader(input_dir=\"../data/lab03/amazon/\").load_data()\n",
    "\n",
    "# build index and query engine\n",
    "vector_query_engine = VectorStoreIndex.from_documents(\n",
    "    amazon_secfiles,\n",
    "    use_async=True,\n",
    ").as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205e3802",
   "metadata": {},
   "source": [
    "SubQuestionQueryEngine will use the LLM model (Claude3 Sonnet) to breaking down of complex queries into sub-queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414cda3-8a56-44a8-96a8-522c0f0e10b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# setup base query engine as tool\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=vector_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"Amazon-10k\",\n",
    "            description=\"Amazon SEC 10-k filings for years 2022 and 2023\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0e68ef-3065-416f-8555-711897998c7f",
   "metadata": {},
   "source": [
    "### > Test Question One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826956e5-2206-4ba0-8aeb-beafa9a284b2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What were key challenges faced by Amazon in year 2022 and 2023?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d542a61-8fec-4af0-9a1c-9b07bacf2f5e",
   "metadata": {},
   "source": [
    "### > Final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aef3a8-1ae8-4d63-8451-8c78e942f572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33dd81-6974-4c13-a282-18b18d8747ee",
   "metadata": {},
   "source": [
    "### > Programmatically iterate through SUB_QUESTION event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74d861-a412-4b59-81c3-51c27120229b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# iterate through sub_question items captured in SUB_QUESTION event\n",
    "from llama_index.core.callbacks import CBEventType, EventPayload\n",
    "\n",
    "for i, (start_event, end_event) in enumerate(\n",
    "    llama_debug.get_event_pairs(CBEventType.SUB_QUESTION)\n",
    "):\n",
    "    qa_pair = end_event.payload[EventPayload.SUB_QUESTION]\n",
    "    print(\"Sub Question \" + str(i) + \": \" + qa_pair.sub_q.sub_question.strip())\n",
    "    print(\"Answer: \" + qa_pair.answer.strip())\n",
    "    print(\"====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b203a-3f9f-46b3-a9b5-f3dd064473a1",
   "metadata": {},
   "source": [
    "### > Test Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbdfaa-ee3b-458c-baf8-df5cae26865a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = query_engine.query(\n",
    "    \"What are amazons key priorities before, during and after covid?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d042f24f-851f-43b9-9435-4887e3237102",
   "metadata": {},
   "source": [
    "### > Final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20f4918-90b3-4ba3-b1eb-be2a086ab500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d71a5c-3ee9-4bf2-848b-bb2422be7400",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

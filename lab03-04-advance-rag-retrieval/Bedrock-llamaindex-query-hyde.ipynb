{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cea58c-48bc-4af6-8358-df9695659983",
   "metadata": {},
   "source": [
    "# RAG Retrieval Optimization - Query Transformation (Hyde) using Amazon Bedrock and Llamaindex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673df1fe-eb6c-46ea-9a73-a96e7ae7942e",
   "metadata": {},
   "source": [
    "HyDE is a special type of query transformation technique that enhances the retrieval process and improve the relevance of retrieved documents. Instead of directly using the original query for retrieval, HyDE leverages a language model to generate a hypothetical document that captures the essence of the query's intent. This hypothetical document is then converted into an embedding, which is used to search for similar real documents in the knowledge base. The underlying concept is that the hypothetical document may be closer in the embedding space to relevant information than the original query itself, potentially leading to more accurate and contextually appropriate retrievals. HyDE has shown promising results in improving RAG performance, especially in zero-shot, and it has demonstrated effectiveness across various languages and tasks.\n",
    "\n",
    "\n",
    "This example is built on referece llamaindex documentation available at - https://github.com/run-llama/llama_index/blob/main/docs/docs/examples/query_transformations/HyDEQueryTransformDemo.ipynb\n",
    "\n",
    "- Vector Database (Faiss / local)\n",
    "- LLM (Amazon Bedrock - Claude3 Sonnet)\n",
    "- Embeddings Model (Bedrock Titan Text Embedding v2.0)\n",
    "- Datasets ( Amazons SEC-10k statments )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67af8141",
   "metadata": {},
   "source": [
    "### > Setup\n",
    "\n",
    "We start by importing necessary llamaindex libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffebb89a-58e0-4fa0-ba8c-fdceb8f09d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05622867",
   "metadata": {},
   "source": [
    "We select Anthropic Claude3 Sonnet as our LLM. For embedding model, we are selecting Amazon Titan Text Embed v2.0. Chunk size is set at 128 for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d47283b-025e-4874-88ed-76245b22f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Sequence, List\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding, Models\n",
    "\n",
    "llm = Bedrock(model = \"anthropic.claude-3-sonnet-20240229-v1:0\")\n",
    "embed_model = BedrockEmbedding(model = \"amazon.titan-embed-text-v2:0\")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.core.tools import BaseTool, FunctionTool\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c98e58",
   "metadata": {},
   "source": [
    "We ingest and index the data stored in data directory. The amazon folder has SEC-10k files from 2022 and 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edde11c-3484-4e21-9be8-56a2ba0ef2a3",
   "metadata": {},
   "source": [
    "### > Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16457565-4547-4655-a695-6e468286bf56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "amazon_secfiles = SimpleDirectoryReader(input_dir=\"../data/lab03/amazon/\").load_data()\n",
    "\n",
    "# build index and query engine\n",
    "index = VectorStoreIndex.from_documents(amazon_secfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765c51e-00b5-4ce9-93b4-5d321ca770e0",
   "metadata": {},
   "source": [
    "## Test query without HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f414cda3-8a56-44a8-96a8-522c0f0e10b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_str = \"What were key challenges faced by Amazon in year 2022 and 2023?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826956e5-2206-4ba0-8aeb-beafa9a284b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b53ed-3bfb-4c9d-a937-d82b78167a29",
   "metadata": {},
   "source": [
    "### > Final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aef3a8-1ae8-4d63-8451-8c78e942f572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7e2a6a",
   "metadata": {},
   "source": [
    "## Test same query with HyDE enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74d861-a412-4b59-81c3-51c27120229b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "hyde_query_engine = TransformQueryEngine(query_engine, hyde)\n",
    "response = hyde_query_engine.query(query_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b5cec-9607-4c08-9f47-7fd42dbae236",
   "metadata": {},
   "source": [
    "### > Response with HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dbdfaa-ee3b-458c-baf8-df5cae26865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1aedc7-0f12-4f6c-8ac3-46a0f72a7a34",
   "metadata": {},
   "source": [
    "### > Example hypothetical question generated by HYDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4fbdad-54dc-4289-8a92-0be3db1aa72e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_bundle = hyde(query_str)\n",
    "hyde_doc = query_bundle.embedding_strs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cd39b-6c0a-4871-b710-2c345352086b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(hyde_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
